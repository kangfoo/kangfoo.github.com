<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[kangfoo's blog]]></title>
  <link href="http://kangfoo.u.qiniudn.com//atom.xml" rel="self"/>
  <link href="http://kangfoo.u.qiniudn.com//"/>
  <updated>2014-03-05T01:02:11+08:00</updated>
  <id>http://kangfoo.u.qiniudn.com//</id>
  <author>
    <name><![CDATA[kangfoo]]></name>
    <email><![CDATA[baseo4233@126.com]]></email>
  </author>
  <generator uri="http://press.opoo.org/">OpooPress</generator>

  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce 学习参考博客汇总]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop--xue-xi-can-kao-bo-ke-hui-zong/"/>
    <updated>2014-03-05T01:01:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop--xue-xi-can-kao-bo-ke-hui-zong/</id>
    <content type="html"><![CDATA[<p>TODO.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop Pipes & Streaming]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-pipes--streaming/"/>
    <updated>2014-03-03T22:26:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-pipes--streaming/</id>
    <content type="html"><![CDATA[<p>申明：本文大部分出自于 <a href="http://new.osforce.cn/?mu=20140227220525KZol8ENMYdFQ6SjMveU26nEZ">开源力量</a> LouisT 老师的<a href="http://new.osforce.cn/course/101?mc101=20140301233857au7XG16o9ukfev1pmFCOfv2s">开源力量培训课-Hadoop Development</a>课件 和 Apache 官方文档。</p>
<h2>Streaming</h2>
<ul>
<li>Streaming 是 hadoop 里面提供的一个工具</li>
<li>Streaming 框架允许任何程序语言实现的程序在 Hadoop MapReduce 中使用，方便任何程序向 Hadoop 平台移植，具有很强的扩展性；</li>
<li>mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程；</li>
<li>如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切法成行并把每一行提供给可执行文件进程的标准输入。同 时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value，作为 mapper的输出。默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括）作为value。如果没有 tab，整行作为 key 值，value值为null。对于reducer，类似；</li>
</ul>
<h3>Streaming 优点</h3>
<ol>
<li><p>开发效率高，便于移植。Hadoop Streaming 使用 Unix 标准流作为 Hadoop 和应用程序之间的接口。在单机上可按照 cat input | mapper | sort | reducer > output 进行测试，若单机上测试通过，集群上一般控制好内存也可以很好的执行成功。</p>
</li>
<li><p>提高运行效率。对内存要求较高，可用C/C++控制内存。比纯java实现更好。</p>
</li>
</ol>
<h3>Streaming缺点</h3>
<ol>
<li><p>Hadoop Streaming 默认只能处理文本数据，（0.21.0之后可以处理二进制数据）。</p>
</li>
<li><p>Steaming 中的 mapper 和 reducer 默认只能想标准输出写数据，不能方便的多路输出。</p>
</li>
</ol>
<p>更详细内容请参考于： http://hadoop.apache.org/docs/r1.2.1/streaming.html</p>
<pre class='brush:shell'>$HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/wc
</pre><h3>streaming示例</h3>
<p>perl 语言的<a href="https://github.com/kangfoo/hadoop1.study/tree/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/streaming">streaming示例</a> 代码</p>
<pre class='brush:perl'>-rw-rw-r--. 1 hadoop hadoop     48 2月  22 10:47 data
-rw-rw-r--. 1 hadoop hadoop 107399 2月  22 10:41 hadoop-streaming-1.2.1.jar
-rw-rw-r--. 1 hadoop hadoop    186 2月  22 10:45 mapper.pl
-rw-rw-r--. 1 hadoop hadoop    297 2月  22 10:55 reducer.pl
##
$ ../bin/hadoop jar hadoop-streaming-1.2.1.jar -mapper mapper.pl -reducer reducer.pl -input /test/streaming -output /test/streamingout1 -file mapper.pl -file reducer.pl 
</pre><h2>Hadoop pipes</h2>
<ol>
<li>Hadoop pipes 是 Hadoop MapReduce 的 C++ 的接口代称。不同于使用标准输入和输出来实现 map 代码和 reduce 代码之间的 Streaming。</li>
<li>Pipes 使用套接字 socket 作为 tasktracker 与 C++ 版本函数的进程间的通讯，未使用 JNI。</li>
<li>与 Streaming 不同，Pipes 是 Socket 通讯，Streaming 是标准输入输出。</li>
</ol>
<h3>编译 Hadoop Pipes</h3>
<p>编译c++ pipes( 确保操作系统提前安装好了 openssl,zlib,glib,openssl-devel)
Hadoop更目录下执行
ant -Dcompile.c++=yes examples</p>
<p>具体请参见《Hadoop Pipes 编译》</p>
<h3>Hadoop官方示例：</h3>
<pre class='brush:shell'>hadoop/src/examples/pipes/impl
 config.h.in
 sort.cc
wordcount-nopipe.cc
wordcount-part.cc
wordcount-simple.cc
</pre><p>运行前需要把可执行文件和输入数据上传到 hdfs：</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -mkdir /test/pipes/input
$ ./bin/hadoop fs -put a.txt /test/pipes/input 
$ ./bin/hadoop fs -cat /test/pipes/input/a.txt 
hello hadoop hello hive hello hbase hello zk
</pre><p>上传执行文件，重新命名为/test/pipes/exec</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -put ./build/c++-examples/Linux-amd64-64/bin/wordcount-simple /test/pipes/exec
</pre><p>在编译好的文件夹目录下执行</p>
<pre>$ cd hadoop/build/c++-examples/Linux-amd64-64/bin
$ ../../../../bin/hadoop pipes -Dhadoop.pipes.java.recordreader=true -Dhadoop.pipes.java.recordwriter=true -reduces 4 -input /test/pipes/input -output /test/pipes/input/output1 -program /test/pipes/execs
</pre><p>执行结果如下：</p>
<pre>$ ./bin/hadoop fs -cat /test/pipes/input/output1/part-00000 hbase 1 
$ ./bin/hadoop fs -cat /test/pipes/input/output1/part-00001 hello 4 hive 1 
$ ./bin/hadoop fs -cat /test/pipes/input/output1/part-00002 hadoop 1 zk 1 
$ ./bin/hadoop fs -cat /test/pipes/input/output1/part-00003
</pre><h3>参考博客：</h3>
<ul>
<li><a href="http://dongxicheng.org/mapreduce/hadoop-pipes-programming/">Hadoop pipes编程</a></li>
<li><a href="http://hongweiyi.com/2012/05/hadoop-pipes-src/">Hadoop Pipes运行机制</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce Sort]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-sort/"/>
    <updated>2014-03-03T22:24:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-sort/</id>
    <content type="html"><![CDATA[<p>排序是 MapReduce 的核心。排序可分为四种排序：普通排序、部分排序、全局排序、辅助排序</p>
<h2>普通排序</h2>
<p>Mapreduce 本身自带排序功能；Text 对象是不适合排序的；IntWritable，LongWritable 等实现了WritableComparable 类型的对象都是可以排序的。</p>
<h2>部分排序</h2>
<p>map 和 reduce 处理过程中包含了默认对 key 的排序，那么如果不要求全排序，可以直接把结果输出，每个输出文件中包含的就是按照key执行排序的结果。</p>
<h3>控制排序顺序</h3>
<p>键的排序是由 RawComparator 控制的，规则如下：</p>
<ol>
<li>若属性 mapred.output.key.comparator.class 已设置，则使用该类的实例。调用 JobConf 的 setOutputKeyComparatorClass() 方法进行设置。</li>
<li>否则，键必须是 WritableComparable 的子类，并使用针对该键类的已登记的 comparator.</li>
<li>如果没有已登记的 comparator ,则使用 RawComparator 将字节流反序列化为一个对象，再由 WritableComparable 的 compareTo() 方法进行操作。</li>
</ol>
<h2>全局排序（对所有数据排序）</h2>
<p>Hadoop 没有提供全局数据排序,而全局排序是非常普遍的需求。</p>
<h3>实现方案</h3>
<ul>
<li>首先，创建一系列的排好序的文件；</li>
<li>其次，串联这些文件；</li>
<li>最后，生成一个全局排序的文件。</li>
</ul>
<p>主要思路是使用一个partitioner来描述全局排序的输出。该方法关键在于如何划分各个分区。</p>
<p>例，对整数排序，[0,10000] 的在 partition 0 中，(10000，20000] 在 partition 1 中… …即第n个reduce 所分配到的数据全部大于第 n-1 个 reduce 中的数据。每个 reduce 的结果都是有序的。</br>
然后再将所有的输出文件顺序合并成一个大的文件，那么就实现了全局排序。</p>
<p>在比较理想的数据分布均匀的情况下，每个分区内的数据量要基本相同。</p>
<p>但实际中数据往往分布不均匀，出现数据倾斜，这时按照此方法进行的分区划分数据就不适用，可对数据进行采样。</p>
<h3>采样器</h3>
<p>通过对 key 空间进行采样，可以较为均匀的划分数据集。采样的核心思想是只查看一小部分键，获取键的相似分布，并由此构建分区。采样器是在 map 阶段之前进行的, 在提交 job 的 client 端完成的。</p>
<h4>Sampler接口</h4>
<p>Sampler 接口是 Hadoop 的采样器，它的 getSample() 方法返回一组样本。此接口一般不由客户端调用，而是由 InputSampler 类的静态方法 writePartitionFile() 调用，以创建一个顺序文件来存储定义分区的键。</p>
<p>Sampler接口声明如下：</p>
<pre class='brush:java'>  public interface Sampler&lt;K,V&gt; {
   K[] getSample(InputFormat&lt;K,V&gt; inf, JobConf job) throws IOException;
  }
</pre><p>继承 Sample 的类还有 IntervalSampler 间隔采样器，RandomSampler 随机采样器，SplitSampler 分词采样器。它们都是 InputSampler 的静态内部类。</p>
<p>getSample() 方法根据 job 的配置信息以及输入格式获得抽样结果，三个采样类各自有不同的实现。</p>
<p><strong>IntervalSampler 根据一定的间隔从 s 个分区中采样数据，非常适合对排好序的数据采样。</strong></p>
<pre class='brush:java'>public static class IntervalSampler&lt;K,V&gt; implements Sampler&lt;K,V&gt; {
    private final double freq;// 哪一条记录被选中的概率
    private final int maxSplitsSampled;// 采样的最大分区数
    /**
     * For each split sampled, emit when the ratio of the number of records
     * retained to the total record count is less than the specified
     * frequency.
     */
    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
    public K[] getSample(InputFormat&lt;K,V&gt; inf, JobConf job) throws IOException {
      InputSplit[] splits = inf.getSplits(job, job.getNumMapTasks());// 1. 得到输入分区数组
      ArrayList&lt;K&gt; samples = new ArrayList&lt;K&gt;();
      int splitsToSample = Math.min(maxSplitsSampled, splits.length);
      int splitStep = splits.length / splitsToSample; // 2. 分区采样时的间隔splitStep = 输入分区总数 除以 splitsToSample的 商；
      long records = 0;
      long kept = 0;
      for (int i = 0; i &lt; splitsToSample; ++i) {
        RecordReader&lt;K,V&gt; reader = inf.getRecordReader(splits[i * splitStep], // 3. 采样下标为i * splitStep的数据
            job, Reporter.NULL);
        K key = reader.createKey();
        V value = reader.createValue();
        while (reader.next(key, value)) {// 6. 循环读取下一条记录
          ++records;
          if ((double) kept / records &lt; freq) { // 4. 如果当前样本数与已经读取的记录数的比值小于freq，则将这条记录添加到样本集合
            ++kept;
            samples.add(key);// 5. 将记录添加到样本集合中
            key = reader.createKey();
          }
        }
        reader.close();
      }
      return (K[])samples.toArray();
    }
  }
… … 
}
</pre><p><strong>RandomSampler 是常用的采样器，它随机地从输入数据中抽取 Key</strong>。</p>
<pre class='brush:java'>  public static class RandomSampler&lt;K,V&gt; implements Sampler&lt;K,V&gt; {
    private double freq;// 一个Key被选中的 概率
    private final int numSamples;// 从所有被选中的分区中获得的总共的样本数目
    private final int maxSplitsSampled;// 需要检查扫描的最大分区数目
/**
     * Randomize the split order, then take the specified number of keys from
     * each split sampled, where each key is selected with the specified
     * probability and possibly replaced by a subsequently selected key when
     * the quota of keys from that split is satisfied.
     */
    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
    public K[] getSample(InputFormat&lt;K,V&gt; inf, JobConf job) throws IOException {
      InputSplit[] splits = inf.getSplits(job, job.getNumMapTasks());// 1. 获取所有的输入分区
      ArrayList&lt;K&gt; samples = new ArrayList&lt;K&gt;(numSamples);// 2. 确定需要抽样扫描的分区数目
      int splitsToSample = Math.min(maxSplitsSampled, splits.length);// 3. 取最小的为采样的分区数
      Random r = new Random();
      long seed = r.nextLong();
      r.setSeed(seed);
      LOG.debug("seed: " + seed);
      // shuffle splits 4. 对输入分区数组shuffle排序
      for (int i = 0; i &lt; splits.length; ++i) {
        InputSplit tmp = splits[i];
        int j = r.nextInt(splits.length);// 5. 打乱其原始顺序
        splits[i] = splits[j];
        splits[j] = tmp;
      }
      // our target rate is in terms of the maximum number of sample splits,
      // but we accept the possibility of sampling additional splits to hit
      // the target sample keyset
// 5. 然后循环逐 个扫描每个分区中的记录进行采样，
      for (int i = 0; i &lt; splitsToSample ||
                     (i &lt; splits.length &amp;&amp; samples.size() &lt; numSamples); ++i) {
        RecordReader&lt;K,V&gt; reader = inf.getRecordReader(splits[i], job,
            Reporter.NULL);
       // 6. 取出一条记录
        K key = reader.createKey();
        V value = reader.createValue();
        while (reader.next(key, value)) {
          if (r.nextDouble() &lt;= freq) {
            if (samples.size() &lt; numSamples) {// 7. 判断当前的采样数是否小于最大采样数
              samples.add(key); //8. 小于则这条记录被选中，放进采样集合中，
            } else {
              // When exceeding the maximum number of samples, replace a
              // random element with this one, then adjust the frequency
              // to reflect the possibility of existing elements being
              // pushed out
              int ind = r.nextInt(numSamples);// 9. 从[0，numSamples]中选择一个随机数
              if (ind != numSamples) {
                samples.set(ind, key);// 10. 替换掉采样集合随机数对应位置的记录，
              }
              freq *= (numSamples - 1) / (double) numSamples;// 11. 调小频率
            }
            key = reader.createKey();// 12. 下一条纪录的key
          }
        }
        reader.close();
      }
      return (K[])samples.toArray();// 13. 返回
    }
  }
… … 
}
</pre><p><strong>SplitSampler 从 s 个分区中采样前 n 个记录，是采样随机数据的一种简便方式。</strong></p>
<pre class='brush:java'>  public static class SplitSampler&lt;K,V&gt; implements Sampler&lt;K,V&gt; {
    private final int numSamples;// 最大采样数
    private final int maxSplitsSampled;// 最大分区数
    … … 
    /**
     * From each split sampled, take the first numSamples / numSplits records.
     */
    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
    public K[] getSample(InputFormat&lt;K,V&gt; inf, JobConf job) throws IOException {
      InputSplit[] splits = inf.getSplits(job, job.getNumMapTasks());
      ArrayList&lt;K&gt; samples = new ArrayList&lt;K&gt;(numSamples);
      int splitsToSample = Math.min(maxSplitsSampled, splits.length);// 1. 采样的分区数
      int splitStep = splits.length / splitsToSample; // 2. 分区采样时的间隔 = 分片的长度 与 输入分片的总数的 商
      int samplesPerSplit = numSamples / splitsToSample; // 3. 每个分区的采样数 
      long records = 0;
      for (int i = 0; i &lt; splitsToSample; ++i) {
        RecordReader&lt;K,V&gt; reader = inf.getRecordReader(splits[i * splitStep], // 4.采样下标为i * splitStep的数据
            job, Reporter.NULL);
        K key = reader.createKey();
        V value = reader.createValue();
        while (reader.next(key, value)) {
          samples.add(key);// 5. 将记录添加到样本集合中
          key = reader.createKey();
          ++records;
          if ((i+1) * samplesPerSplit &lt;= records) { // 6. 当前样本数大于当前的采样分区所需要的样本数，则停止对当前分区的采样。
            break;
          }
        }
        reader.close();
      }
      return (K[])samples.toArray();
    }
  }
</pre><p><strong>Hadoop为顺序文件提供了一个 TotalOrderPartitioner 类，可以用来实现全局排序</strong>；TotalOrderPartitioner 源代码理解。TotalOrderPartitioner 内部定义了多个字典树（内部类）。</p>
<pre class='brush:java'>interface Node&lt;T&gt; 
// 特里树，利用字符串的公共前缀来节约存储空间，最大限度地减少无谓的字符串比较，查询效率比哈希表高
static abstract class TrieNode implements Node&lt;BinaryComparable&gt; 
static class InnerTrieNode extends TrieNode 
static class LeafTrieNode extends TrieNode
… … 
</pre><p>由 TotalOrderPartitioner 调用 getPartition() 方法返回分区，由 buildTrieRec() 构建特里树.</p>
<pre class='brush:java'> private TrieNode buildTrieRec(BinaryComparable[] splits, int lower,
      int upper, byte[] prefix, int maxDepth, CarriedTrieNodeRef ref) {
… … 
}
</pre><h4>采样器使用示例</h4>
<ol>
<li>新建文件，名为 random.txt，里面每行存放一个数据。可由 RandomGenerator 类生成准备数据</li>
<li>执行 TestTotalOrderPartitioner.java</li>
</ol>
<h2>辅助排序</h2>
<p>先按 key 排序，在按 相同的 key 不同的 value 再排序。可实现对值分组的效果。</p>
<ul>
<li>可参考博客 <a href="http://heipark.iteye.com/blog/1990237">Hadoop二次排序关键点和出现时机（也叫辅助排序、Secondary Sort）</a></li>
<li>或者 hadoop example 工程下参考 SecondarySort.java</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce Join]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-join/"/>
    <updated>2014-03-03T22:23:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-join/</id>
    <content type="html"><![CDATA[<p>在 Hadoop 中可以通过 MapReduce，Pig，hive，Cascading编程进行大型数据集间的连接操作。连接操作如果由 Mapper 执行，则称为“map端连接”；如果由 Reduce 执行，则称为“Reduce端连接”。</p>
<p>连接操作的具体实现技术取决于数据集的规模以及分区方式。</br>
若一个数据集很大而另一个数据集很小，以至于可以分发到集群中的每一个节点之中，则可以执行一个 MapReduce 作业，将各个数据集的数据放到一起，从而实现连接。</br>
若两个数据规模均很大，没有哪个数据集可以完全复制到集群的每个节点，可以使用 MapReduce 作业进行连接，使用 Map 端连接还是 Reduce 端连接取决于数据的组织方式。</br></p>
<p>Map端连接将所有的工作在 map 中操作，效率高但是不通用。而 Reduce 端连接利用了 shuff 机制，进行连接，效率不高。</p>
<p>DistributedCache 能够在任务运行过程中及时地将文件和存档复制到任务节点进行本地缓存以供使用。各个文件通常只复制到一个节点一次。可用 api 或者命令行在需要的时候将本地文件添加到 hdfs 文件系统中。</p>
<p>本文中的示例 <strong>出自于 <a href="http://new.osforce.cn/?mu=20140227220525KZol8ENMYdFQ6SjMveU26nEZ">开源力量</a> LouisT 老师的<a href="http://new.osforce.cn/course/101?mc101=20140301233857au7XG16o9ukfev1pmFCOfv2s">开源力量培训课-Hadoop Development</a>课件。</strong></p>
<h3>Map端连接</h3>
<p>Map 端联接是指数据到达 map 处理函数之前进行合并的。它要求 map 的输入数据必须先分区并以特定的方式排序。各个输入数据集被划分成相同数量的分区，并均按相同的键排序（连接键）。同一键的所有输入纪录均会放在同一个分区。以满足 MapReduce 作业的输出。</p>
<p>若作业的 Reduce 数量相同、键相同、输入文件是不可切分的，那么 map 端连接操作可以连接多个作业的输出。</p>
<p>在 Map 端连接效率比 Reduce 端连接效率高（Reduce端Shuff耗时），但是要求比较苛刻。</p>
<h4>基本思路</h4>
<ol>
<li>将需要 join 的两个文件，一个存储在 HDFS 中，一个使用 DistributedCache.addCacheFile() 将需要 join 另一个文件加入到所有 Map 的缓存里(DistributedCache.addCacheFile() 需要在作业提交前设置)；</li>
<li>在 Map 函数里读取该文件，进行 Join；</li>
<li>将结果输出到 reduce 端；</li>
</ol>
<h4>使用步骤</h4>
<ol>
<li>在 HDFS 中上传文件（文本文件、压缩文件、jar包等）；</li>
<li>调用相关API添加文件信息；</li>
<li>task运行前直接调用文件读写API获取文件；</li>
</ol>
<h3>Reduce端Join</h3>
<p>reduce 端联接比 map 端联接更普遍，因为输入的数据不需要特定的结构；效率低（所有数据必须经过shuffle过程）。</p>
<h4>基本思路</h4>
<ol>
<li>Map 端读取所有文件，并在输出的内容里加上标识代表数据是从哪个文件里来的；</li>
<li>在 reduce 处理函数里，对按照标识对数据进行保存；</li>
<li>然后根据 Key 的 Join 来求出结果直接输出；</li>
</ol>
<h3>示例程序</h3>
<p>使用 MapReduce map 端join 或者 reduce 端 join 实现如下两张表 emp, dep 中的 SQL 联合查询的数据效果。</p>
<pre class='brush:text'>Table EMP：（新建文件EMP，第一行属性名不要）
----------------------------------------
Name      Sex      Age     DepNo
zhang      male     20           1     
li              female  25           2
wang       female  30           3
zhou        male     35           2
----------------------------------------
Table Dep：（新建文件DEP，第一行属性名不要）
DepNo     DepName
     1            Sales
     2            Dev
     3            Mgt
------------------------------------------------------------     
SQL：
select name,sex ,age, depName from emp inner join DEP on EMP.DepNo = Dep.DepNo
----------------------------------------
实现效果：
$ ./bin/hadoop fs -cat /reduceSideJoin/output11/part-r-00000
zhang male 20 sales
li female 25 dev
wang female 30 dev
zhou male 35 dev
</pre><p>Map 端 Join 的例子：<a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/join/TestMapSideJoin.java">TestMapSideJoin</a> </br>
Reduce 端 Join 的例子：<a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/join/TestReduceSideJoin.java">TestReduceSideJoin</a> </br></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce 计数器]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--ji-shu-qi/"/>
    <updated>2014-03-03T22:22:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--ji-shu-qi/</id>
    <content type="html"><![CDATA[<p>计数器是一种收集系统信息有效手段，用于质量控制或应用级统计。可辅助诊断系统故障。计数器可以比日志更方便的统计事件发生次数。</p>
<h3>内置计数器</h3>
<p>Hadoop 为每个作业维护若干内置计数器，主要用来记录作业的执行情况。</p>
<h4>内置计数器包括</h4>
<ul>
<li>MapReduce 框架计数器（Map-Reduce Framework）</li>
<li>文件系统计数器（FielSystemCounters）</li>
<li>作业计数器（Job Counters）</li>
<li>文件输入格式计数器（File Output Format Counters）</li>
<li>文件输出格式计数器（File Input Format Counters）</li>
</ul>
<!--表格数据太多，暂缓。 TODO-->
<!--<div  style="height:0px;border-bottom:1px dashed red"></div>
<table width="100%" border="1" cellpadding="3"  cellspacing="0" bordercolor="#eeeeee">
<tbody>
<tr>
    <td><em>组别 </em></td>
    <td><em>计数器名称 </em></td>
    <td><em>说明 </em></td>
</tr>
<tr>
    <th rowspan=13>Map-Reduce <br>Framework </th>
    <td>Map input records </td>
    <td>作业中所有的 map 已处理的输入纪录数。每次 RecordReader 读到一条纪录并将其传递给 map 的 map() 函数时，此计数器的值增加 </td>
</tr>
<tr>
    <td>Map skipped records </td>
    <td>作业中所有 map 跳过的输入纪录数。 </td>
</tr>
</tbody>
</table>
-->
<p>计数器由其关联的 task 进行维护，定期传递给 tasktracker，再由 tasktracker 传给 jobtracker。因此，计数器能够被全局地聚集。内置计数器实际由 jobtracker 维护，不必在整个网络发送。</p>
<p>一个任务的计数器值每次都是完整传输的，仅当一个作业执行成功之后，计数器的值才完整可靠的。</p>
<h3>自定义Java计数器</h3>
<p>MapReduce 允许用户自定义计数器，MapReduce 框架将跨所有 map 和 reduce 聚集这些计数器，并在作业结束的时候产生一个最终的结果。</p>
<p>计数器的值可以在 mapper 或者 reducer 中添加。多个计数器可以由一个 java 枚举类型来定义，以便对计数器分组。一个作业可以定义的枚举类型数量不限，个个枚举类型所包含的数量也不限。</p>
<p>枚举类型的名称即为组的名称，枚举类型的字段即为计数器名称。</p>
<p>在 TaskInputOutputContext 中的 counter</p>
<pre class='brush:java'> public Counter getCounter(Enum&lt;?&gt; counterName) {
    return reporter.getCounter(counterName);
  }
  public Counter getCounter(String groupName, String counterName) {
    return reporter.getCounter(groupName, counterName);
  }
</pre><h4>计数器递增</h4>
<p>org.apache.hadoop.mapreduce.Counter类</p>
<pre class='brush:java'>  public synchronized void increment(long incr) {
    value += incr;
  }
</pre><h4>计数器使用</h4>
<ul>
<li>WebUI 查看（50030）；</li>
<li>命令行方式：hadoop job [-counter <job-id> <group-name> <counter-name>]；</li>
<li>使用Hadoop API。
通过job.getCounters()得到Counters,而后调用counters.findCounter()方法去得到计数器对象；可参见《Hadoop权威指南》第8章 示例 8-2 MissingTemperaureFields.java</li>
</ul>
<h4>命令行方式示例</h4>
<pre class='brush:shell'>$ ./bin/hadoop job -counter  job_201402211848_0004 FileSystemCounters HDFS_BYTES_READ
177
</pre><h3>自定义计数器</h3>
<p>统计词汇行中词汇数超过2个或少于2个的行数。 源代码： <a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/counter/TestCounter.java">TestCounter.java</a>TestCounter.java</p>
<h4>输入数据文件值 counter.txt:</h4>
<pre class='brush:text'>hello world
hello
hello world 111
hello world 111 222
</pre><p>执行参数</p>
<pre class='brush:java'>hdfs://master11:9000/counter/input/a.txt hdfs://master11:9000/counter/output1
</pre><p>计数器统计(hadoop eclipse 插件执行)结果：</p>
<pre class='brush:shell'>2014-02-21 00:03:38,676 INFO  mapred.JobClient (Counters.java:log(587)) -   ERROR_COUNTER
2014-02-21 00:03:38,677 INFO  mapred.JobClient (Counters.java:log(589)) -     Above_2=2
2014-02-21 00:03:38,677 INFO  mapred.JobClient (Counters.java:log(589)) -     BELOW_2=1
</pre>]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce RecordReader 组件]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-recordreader-zu-jian/"/>
    <updated>2014-03-03T22:21:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-recordreader-zu-jian/</id>
    <content type="html"><![CDATA[<p>由 RecordReader 决定每次读取以什么样的方式读取数据分片中的一条数据。Hadoop 默认的 RecordReader 是 LineRecordReader（TextInputFormat 的 getRecordReader() 方法返回即是 LineRecordReader。二进制输入 SequenceFileInputFormat 的 getRecordReader() 方法返回即是SequenceFileRecordReader。）。LineRecordReader是用每行的偏移量作为 map 的 key，每行的内容作为 map 的 value；</p>
<p>它可作用于，自定义读取每一条记录的方式；自定义读入 key 的类型，如希望读取的 key 是文件的路径或名字而不是该行在文件中的偏移量。</p>
<h3>自定义RecordReader一般步骤</h3>
<ol>
<li>继承抽象类 RecordReader，实现 RecordReader 的实例；</li>
<li>实现自定义 InputFormat 类，重写 InputFormat 中 createRecordReader() 方法，返回值是自定义的 RecordReader 实例；
（3）配置 job.setInputFormatClass() 设置自定义的 InputFormat 类型；</li>
</ol>
<h3>TextInputFormat类源代码理解</h3>
<p>源码见 org.apache.mapreduce.lib.input.TextInputFormat 类(新API)；</p>
<p>Hadoop 默认 TextInputFormat 使用 LineRecordReader。具体分析见注释。</p>
<pre class='brush:java'>  public RecordReader&lt;LongWritable, Text&gt; 
    createRecordReader(InputSplit split,
                       TaskAttemptContext context) {
    return new LineRecordReader();
  }
// --&gt; LineRecordReader
 public void initialize(InputSplit genericSplit,
                         TaskAttemptContext context) throws IOException {
    FileSplit split = (FileSplit) genericSplit;
    Configuration job = context.getConfiguration();
    this.maxLineLength = job.getInt("mapred.linerecordreader.maxlength",
                                    Integer.MAX_VALUE);
    start = split.getStart();  // 当前分片在整个文件中的起始位置
    end = start + split.getLength(); // 当前分片，在整个文件的位置
    final Path file = split.getPath();
    compressionCodecs = new CompressionCodecFactory(job);// 压缩
    codec = compressionCodecs.getCodec(file);
//
    // open the file and seek to the start of the split
    FileSystem fs = file.getFileSystem(job);
    FSDataInputStream fileIn = fs.open(split.getPath()); // 获取 FSDataInputStream
//
    if (isCompressedInput()) {
      decompressor = CodecPool.getDecompressor(codec);
      if (codec instanceof SplittableCompressionCodec) {
        final SplitCompressionInputStream cIn =
          ((SplittableCompressionCodec)codec).createInputStream(
            fileIn, decompressor, start, end,
            SplittableCompressionCodec.READ_MODE.BYBLOCK);
        in = new LineReader(cIn, job); //一行行读取
        start = cIn.getAdjustedStart(); // 可能跨分区读取
        end = cIn.getAdjustedEnd();// 可能跨分区读取
        filePosition = cIn;
      } else {
        in = new LineReader(codec.createInputStream(fileIn, decompressor),
            job);
        filePosition = fileIn;
      }
    } else {
      fileIn.seek(start);//  调整到文件起始偏移量
      in = new LineReader(fileIn, job); 
      filePosition = fileIn;
    }
    // If this is not the first split, we always throw away first record
    // because we always (except the last split) read one extra line in
    // next() method.
    if (start != 0) {
      start += in.readLine(new Text(), 0, maxBytesToConsume(start));
    }
    this.pos = start; // 在当前分片的位置
  }
//  --&gt; getFilePosition() 指针读取到哪个位置
// filePosition 为 Seekable 类型
  private long getFilePosition() throws IOException {
    long retVal;
    if (isCompressedInput() &amp;&amp; null != filePosition) {
      retVal = filePosition.getPos();
    } else {
      retVal = pos;
    }
    return retVal;
  }
//
// --&gt; nextKeyValue() 
public boolean nextKeyValue() throws IOException {
    if (key == null) {
      key = new LongWritable();
    }
    key.set(pos);
    if (value == null) {
      value = new Text();
    }
    int newSize = 0;
    // We always read one extra line, which lies outside the upper
    // split limit i.e. (end - 1)
    // 预读取下一条纪录
    while (getFilePosition() &lt;= end) {
      newSize = in.readLine(value, maxLineLength,
          Math.max(maxBytesToConsume(pos), maxLineLength));
      if (newSize == 0) {
        break;
      }
      pos += newSize; // 下一行的偏移量
      if (newSize &lt; maxLineLength) {
        break;
      }
//
      // line too long. try again
      LOG.info("Skipped line of size " + newSize + " at pos " + 
               (pos - newSize));
    }
    if (newSize == 0) {
      key = null;
      value = null;
      return false;
    } else {
      return true;
    }
  }
</pre><h3>自定义 RecordReader 演示</h3>
<p>假设，现有如下数据 10 ～ 70 需要利用自定义 RecordReader 组件分别计算数据奇数行和偶数行的数据之和。结果为：奇数行之和等于 160，偶数和等于 120。<strong>出自于 <a href="http://new.osforce.cn/?mu=20140227220525KZol8ENMYdFQ6SjMveU26nEZ">开源力量</a> LouisT 老师的<a href="http://new.osforce.cn/course/101?mc101=20140301233857au7XG16o9ukfev1pmFCOfv2s">开源力量培训课-Hadoop Development</a>课件。</strong></p>
<p>数据：</br>
10</br>
20</br>
30</br>
40</br>
50</br>
60</br>
70</br></p>
<h4>源代码</h4>
<p><a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/typeformat/TestRecordReader.java">TestRecordReader.java</a></p>
<h4>数据准备</h4>
<pre class='brush:shell'>$ ./bin/hadoop fs -mkdir /inputreader
$ ./bin/hadoop fs -put ./a.txt /inputreader
$ ./bin/hadoop fs -lsr /inputreader
-rw-r--r--   2 hadoop supergroup         21 2014-02-20 21:04 /inputreader/a.txt
</pre><h4>执行</h4>
<pre class='brush:shell'>$ ./bin/hadoop jar study.hdfs-0.0.1-SNAPSHOT.jar TestRecordReader  /inputreader /inputreaderout1
##
$ ./bin/hadoop fs -lsr /inputreaderout1
-rw-r--r--   2 hadoop supergroup          0 2014-02-20 21:12 /inputreaderout1/_SUCCESS
drwxr-xr-x   - hadoop supergroup          0 2014-02-20 21:11 /inputreaderout1/_logs
drwxr-xr-x   - hadoop supergroup          0 2014-02-20 21:11 /inputreaderout1/_logs/history
-rw-r--r--   2 hadoop supergroup      16451 2014-02-20 21:11 /inputreaderout1/_logs/history/job_201402201934_0002_1392901901142_hadoop_TestRecordReader
-rw-r--r--   2 hadoop supergroup      48294 2014-02-20 21:11 /inputreaderout1/_logs/history/job_201402201934_0002_conf.xml
-rw-r--r--   2 hadoop supergroup         23 2014-02-20 21:12 /inputreaderout1/part-r-00000
-rw-r--r--   2 hadoop supergroup         23 2014-02-20 21:12 /inputreaderout1/part-r-00001
##
$ ./bin/hadoop fs -cat /inputreaderout1/part-r-00000
偶数行之和：  120
##
$ ./bin/hadoop fs -cat /inputreaderout1/part-r-00001
奇数行之和：  160
</pre>]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce Partitioner 组件]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-partitioner--zu-jian/"/>
    <updated>2014-03-03T22:20:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-partitioner--zu-jian/</id>
    <content type="html"><![CDATA[<p>Partitioner 过程发生在循环缓冲区发生溢写文件之后，merge 之前。可以让 Map 对 Key 进行分区，从而可以根据不同的 key 来分发到不同的 reducer 中去处理；</p>
<p>Hadoop默认的提供的是HashPartitioner。</p>
<p>可以自定义 key 的分发规则，自定义Partitioner：</p>
<ul>
<li>继承抽象类Partitioner，实现自定义的getPartition（）方法；</li>
<li>通过job.setPartitionerClass（）来设置自定义的Partitioner；</li>
</ul>
<h3>Partitioner 类</h3>
<p>旧api</p>
<pre class='brush:java'>public interface Partitioner&lt;K2, V2&gt; extends JobConfigurable {
  int getPartition(K2 key, V2 value, int numPartitions);
}
</pre><p>新api</p>
<pre class='brush:java'>public abstract class Partitioner&lt;KEY, VALUE&gt; {
  public abstract int getPartition(KEY key, VALUE value, int numPartitions);  
}
</pre><h3>Partitioner应用场景演示</h3>
<p>需求：利用 Hadoop MapReduce 作业 Partitioner 组件分别统计每种商品的周销售情况。源代码 <a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/typeformat/TestPartitioner.java">TestPartitioner.java</a>。<strong>出自于 <a href="http://new.osforce.cn/?mu=20140227220525KZol8ENMYdFQ6SjMveU26nEZ">开源力量</a> LouisT 老师的<a href="http://new.osforce.cn/course/101?mc101=20140301233857au7XG16o9ukfev1pmFCOfv2s">开源力量培训课-Hadoop Development</a>课件。</strong> (可使用 PM2.5 数据代替此演示程序)</p>
<ul>
<li><p>site1的周销售清单（a.txt,以空格分开）：</p>
<pre class='brush:text'>shoes   20
hat 10
stockings   30
clothes 40
</pre></li>
<li><p>site2的周销售清单（b.txt，以空格分开）：</p>
<pre class='brush:text'>shoes   15
hat 1
stockings   90
clothes 80
</pre></li>
<li><p>汇总结果：</p>
<pre class='brush:text'>shoes     35
hat       11
stockings 120
clothes   120
</pre></li>
<li><p>准备测试数据</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -mkdir /testPartitioner/input
$ ./bin/hadoop fs -put a.txt /testPartitioner/input
$ ./bin/hadoop fs -put b.txt /testPartitioner/input
$ ./bin/hadoop fs -lsr /testPartitioner/input
-rw-r--r--   2 hadoop supergroup         52 2014-02-18 22:53 /testPartitioner/input/a.txt
-rw-r--r--   2 hadoop supergroup         50 2014-02-18 22:53 /testPartitioner/input/b.txt
</pre></li>
<li><p>执行 MapReduce 作业
此处使用 hadoop jar 命令执行，eclipse 插件方式有一定的缺陷。(hadoop eclipse 执行出现java.io.IOException: Illegal partition for hat (1))</p>
<pre class='brush:shell'>$ ./bin/hadoop jar study.hdfs-0.0.1-SNAPSHOT.jar TestPartitioner /testPartitioner/input /testPartitioner/output10
</pre></li>
<li><p>结果。 四个分区，分别存储上述四种产品的总销量的统计结果值。</p>
<pre class='brush:shell'>-rw-r--r--   2 hadoop supergroup          9 2014-02-19 00:18 /testPartitioner/output10/part-r-00000
-rw-r--r--   2 hadoop supergroup          7 2014-02-19 00:18 /testPartitioner/output10/part-r-00001
-rw-r--r--   2 hadoop supergroup         14 2014-02-19 00:18 /testPartitioner/output10/part-r-00002
-rw-r--r--   2 hadoop supergroup         12 2014-02-19 00:18 /testPartitioner/output10/part-r-00003
</pre></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce Combiner 组件]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-combiner--zu-jian/"/>
    <updated>2014-03-03T22:19:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce-combiner--zu-jian/</id>
    <content type="html"><![CDATA[<!-- 参考博客，学习怎么写明了。 http://blog.csdn.net/heyutao007/article/details/5725379-->
<p>combiner 作用是把一个 map 产生的多个 <key,valeu> 合并成一个新的 <key,valeu>，然后再将新的 <key,valeu> 作为 reduce 的输入；</p>
<p>combiner 函数在 map 函数与 reduce 函数之间，目的是为了减少 map 输出的中间结果，减少 reduce 复制 map 输出的数据，减少网络传输负载；</p>
<p>并不是所有情况下都能使用 Combiner 组件，它适用于对记录汇总的场景（如求和，平均数不适用）</p>
<h4>什么时候运行 Combiner</h4>
<ul>
<li>当 job 设置了 Combiner，并且 spill 的个数达到 min.num.spill.for.combine (默认是3)的时候，那么 combiner 就会 Merge 之前执行；</li>
<li>但是有的情况下，Merge 开始执行，但 spill 文件的个数没有达到需求，这个时候 Combiner 可能会在Merge 之后执行；</li>
<li>Combiner 也有可能不运行，Combiner 会考虑当时集群的一个负载情况。</li>
</ul>
<h4>测试 Combinner 过程</h4>
<p>代码 <a href="https://github.com/kangfoo/hadoop1.study/blob/fa6e68e52aa12ed0e22f98e6109f376ffbb6431f/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/typeformat/TestCombiner.java">TestCombiner</a></p>
<ol>
<li><p>以 wordcount.txt 为输入的词频统计</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -lsr /test3/input
drwxr-xr-x   - hadoop supergroup          0 2014-02-18 00:28 /test3/input/test
-rw-r--r--   2 hadoop supergroup        983 2014-02-18 00:28 /test3/input/test/wordcount.txt
-rw-r--r--   2 hadoop supergroup        626 2014-02-18 00:28 /test3/input/test/wordcount2.txt
</pre></li>
<li><p><strong>不启用 Reducer</strong> (输出，字节变大)</p>
<pre class='brush:shell'>drwxr-xr-x   - kangfoo-mac supergroup          0 2014-02-18 00:29 /test3/output1
-rw-r--r--   3 kangfoo-mac supergroup          0 2014-02-18 00:29 /test3/output1/_SUCCESS
-rw-r--r--   3 kangfoo-mac supergroup       1031 2014-02-18 00:29 /test3/output1/part-m-00000 (-m 没有 reduce 过程的中间结果，每个数据文件对应一个数据分片,每个分片对应一个map任务)
-rw-r--r--   3 kangfoo-mac supergroup        703 2014-02-18 00:29 /test3/output1/part-m-00001
</pre><p>结果如下（map过程并不合并相同key的value值）：</p>
<pre class='brush:shell'>drwxr-xr-x  1
-  1
hadoop  1
supergroup  1
0   1
2014-02-17  1
21:03   1
/home/hadoop/env/mapreduce  1
drwxr-xr-x  1
-  1
hadoop  1
</pre></li>
<li><p><strong>启用 Reducer</strong></p>
<pre class='brush:shell'>drwxr-xr-x   - kangfoo-mac supergroup          0 2014-02-18 00:29 /test3/output1
-rw-r--r--   3 kangfoo-mac supergroup          0 2014-02-18 00:29 /test3/output1/_SUCCESS
-rw-r--r--   3 kangfoo-mac supergroup       1031 2014-02-18 00:29 /test3/output1/part-m-00000
-rw-r--r--   3 kangfoo-mac supergroup        703 2014-02-18 00:29 /test3/output1/part-m-00001
drwxr-xr-x   - kangfoo-mac supergroup          0 2014-02-18 00:31 /test3/output2
-rw-r--r--   3 kangfoo-mac supergroup          0 2014-02-18 00:31 /test3/output2/_SUCCESS
-rw-r--r--   3 kangfoo-mac supergroup        705 2014-02-18 00:31 /test3/output2/part-r-00000
</pre><p>结果：</p>
<pre class='brush:text'>0:17:31,680 6
014-02-18   1
2014-02-17  11
2014-02-18  5
21:02   7
</pre></li>
<li><p>在日志或者 http://master11:50030/jobtracker.jsp 页面查找是否执行过 Combine 过程。
日志截取如下：</p>
<pre class='brush:text'>2014-02-18 00:31:29,894 INFO  SPLIT_RAW_BYTES=233
2014-02-18 00:31:29,894 INFO  Combine input records=140
2014-02-18 00:31:29,894 INFO  Reduce input records=43
2014-02-18 00:31:29,894 INFO  Reduce input groups=42
2014-02-18 00:31:29,894 INFO  Combine output records=43
2014-02-18 00:31:29,894 INFO  Reduce output records=42
2014-02-18 00:31:29,894 INFO  Map output records=140
</pre></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce 类型与格式]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--lei-xing-yu-ge-shi/"/>
    <updated>2014-03-03T22:18:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--lei-xing-yu-ge-shi/</id>
    <content type="html"><![CDATA[<p>MapReduce 的 map和reduce函数的输入和输出是键/值对(key/value pair) 形式的数据处理模型。</p>
<h2>MapReduce 的类型</h2>
<p>Hadoop1.x MapReduce 有2套API.旧api偏向与接口，新api偏向与抽象类，如无特殊默认列举为旧的api作讨论.</p>
<p>在Hadoop的MapReduce中，map和reduce函数遵循如下格式：</p>
<ul>
<li>map(K1, V1) –> list (K2, V2)   // map：对输入分片数据进行过滤数据，组织 key/value 对等操作<br  /></li>
<li>combine(K2, list(V2)) –> list(K2, V2) // 在map端对输出进行预处理，类似 reduce。combine 不一定适用任何情况，如：对总和求平均数。选用。</li>
<li>partition(K2, V2) –> integer    // 将中间键值对划分到一个 reduce 分区，返回分区索引号。实际上，分区单独由键决定(值是被忽略的)，分区内的键会排序，相同的键的所有值会合成一个组（list(V2)）<br  /></li>
<li>reduce(K2, list(V2)) –> list(K3, V3) // 每个 reduce 会处理具有某些特性的键，每个键上都有值的序列，是通过对所有 map 输出的值进行统计得来的，reduce 根据所有map传来的结果，最后进行统计合并操作，并输出结果。</li>
</ul>
<p>旧api类代码</p>
<pre class='brush:java'>public interface Mapper&lt;K1, V1, K2, V2&gt; extends JobConfigurable, Closeable {  
  void map(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter) throws IOException;
}
//
public interface Reducer&lt;K2, V2, K3, V3&gt; extends JobConfigurable, Closeable {
  void reduce(K2 key, Iterator&lt;V2&gt; values, OutputCollector&lt;K3, V3&gt; output, Reporter reporter) throws IOException;
}
//
public interface Partitioner&lt;K2, V2&gt; extends JobConfigurable {
   int getPartition(K2 key, V2 value, int numPartitions);
}
</pre><p>新api类代码</p>
<pre class='brush:java'>public class Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; {
… …
  protected void map(KEYIN key, VALUEIN value, 
                     Context context) throws IOException, InterruptedException {
    context.write((KEYOUT) key, (VALUEOUT) value);
  }
… …
}
//
public class Reducer&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; {
… …
 protected void reduce(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context
                        ) throws IOException, InterruptedException {
    for(VALUEIN value: values) {
      context.write((KEYOUT) key, (VALUEOUT) value);
    }
  }
… …
}
//
public interface Partitioner&lt;K2, V2&gt; extends JobConfigurable {
  int getPartition(K2 key, V2 value, int numPartitions);
}
</pre><p>默认的 partitioner 是 HashPartitioner，对键进行哈希操作以决定该记录属于哪个分区让 reduce 处理，每个分区对应一个 reducer 任务。总槽数 solt＝集群中节点数 ＊ 每个节点的任务槽。实际值应该比理论值要小，以空闲一部分在错误容忍是备用。</p>
<p>HashPartitioner的实现</p>
<pre class='brush:java'>public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; {
    public int getPartition(K key, V value, int numReduceTasks) {
        return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;
    }
}
</pre><p>hadooop1.x 版本中</p>
<ul>
<li>旧的api,map 默认的 IdentityMapper, reduce 默认的是 IdentityReducer</li>
<li>新的api,map 默认的 Mapper, reduce 默认的是 Reducer</li>
</ul>
<p>默认MapReduce函数实例程序</p>
<pre class='brush:java'>public class MinimalMapReduceWithDefaults extends Configured implements Tool {
    @Override
    public int run(String[] args) throws Exception {
        Job job = JobBuilder.parseInputAndOutput(this, getConf(), args);
        if (job == null) {
            return -1;
            }
        //
        job.setInputFormatClass(TextInputFormat.class);
        job.setMapperClass(Mapper.class);
        job.setMapOutputKeyClass(LongWritable.class);
        job.setMapOutputValueClass(Text.class);
        job.setPartitionerClass(HashPartitioner.class);
        job.setNumReduceTasks(1);
        job.setReducerClass(Reducer.class);
        job.setOutputKeyClass(LongWritable.class);
        job.setOutputValueClass(Text.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        return job.waitForCompletion(true) ? 0 : 1;
        }
    //
    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new MinimalMapReduceWithDefaults(), args);
        System.exit(exitCode);
        }
}
</pre><h2>输入格式</h2>
<h3>输入分片与记录</h3>
<p>一个输入分片(input split)是由单个 map 处理的输入块，即每一个 map 只处理一个输入分片，每个分片被划分为若干个记录( records )，每条记录就是一个 key/value 对，map 一个接一个的处理每条记录，输入分片和记录都是逻辑的，不必将他们对应到文件上。数据分片由数据块大小决定的。</p>
<p>注意，一个分片不包含数据本身，而是指向数据的引用( reference )。</p>
<p>输入分片在Java中被表示为InputSplit抽象类</p>
<pre class='brush:java'>public interface InputSplit extends Writable {
  long getLength() throws IOException;
  String[] getLocations() throws IOException;
}
</pre><p>InputFormat负责创建输入分片并将它们分割成记录，抽象类如下：</p>
<pre class='brush:java'>public interface InputFormat&lt;K, V&gt; {
  InputSplit[] getSplits(JobConf job, int numSplits) throws IOException;
  RecordReader&lt;K, V&gt; getRecordReader(InputSplit split,
                                     JobConf job, 
                                     Reporter reporter) throws IOException;
}
</pre><p>客户端通过调用 getSpilts() 方法获得分片数目(怎么调到的？)，在 TaskTracker 或 NodeManager上，MapTask 会将分片信息传给 InputFormat 的
createRecordReader() 方法，进而这个方法来获得这个分片的 RecordReader，RecordReader 基本就是记录上的迭代器，MapTask 用一个 RecordReader 来生成记录的 key/value 对，然后再传递给 map 函数，如下步骤：</p>
<ol>
<li>jobClient调用getSpilts()方法获得分片数目，将numSplits作为参数传入，以参考。InputFomat实现有自己的getSplits()方法。</li>
<li>客户端将他们发送到jobtracker</li>
<li>jobtracker使用其存储位置信息来调度map任务从而在tasktracker上处理分片数据</li>
<li>在tasktracker上，map任务把输入分片传给InputFormat上的getRecordReader()方法，来获取分片的RecordReader。</li>
<li>map 用一个RecordReader来生成纪录的键值对。</li>
<li>RecordReader的next()方法被调用，知道返回false。map任务结束。</li>
</ol>
<p>MapRunner 类部分代码（旧api）</p>
<pre class='brush:java'>public class MapRunner&lt;K1, V1, K2, V2&gt;
    implements MapRunnable&lt;K1, V1, K2, V2&gt; {
… … 
 public void run(RecordReader&lt;K1, V1&gt; input, OutputCollector&lt;K2, V2&gt; output,
                  Reporter reporter)
    throws IOException {
    try {
      // allocate key &amp; value instances that are re-used for all entries
      K1 key = input.createKey();
      V1 value = input.createValue();
      //
      while (input.next(key, value)) {
        // map pair to output
        mapper.map(key, value, output, reporter);
        if(incrProcCount) {
          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, 
              SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1);
        }
      }
    } finally {
      mapper.close();
    }
  }
……
}
</pre><h3>FileInputFormat类</h3>
<p>FileInputFormat是所有使用文件为数据源的InputFormat实现的基类，它提供了两个功能：一个定义哪些文件包含在一个作业的输入中；一个为输入文件生成分片的实现，把分片割成记录的作业由其子类来完成。</p>
<p><strong>下图为InputFormat类的层次结构</strong>：
<img src="http://zhaomingtai.u.qiniudn.com/FileInputFormat.png" alt="image" /></p>
<h4>FileInputFormat 类输入路径</h4>
<p>FileInputFormat 提供四种静态方法来设定 Job 的输入路径，其中下面的 addInputPath() 方法  addInputPaths() 方法可以将一个或多个路径加入路径列表，setInputPaths() 方法一次设定完整的路径列表(可以替换前面所设路 径)</p>
<pre class='brush:java'>public static void addInputPath(Job job, Path path);
public static void addInputPaths(Job job, String commaSeparatedPaths);
public static void setInputPaths(Job job, Path... inputPaths);
public static void setInputPaths(Job job, String commaSeparatedPaths);
</pre><p>如果需要排除特定文件，可以使用 FileInputFormat 的 setInputPathFilter() 设置一个过滤器：
<code>public static void setInputPathFilter(Job job, Class&lt;? extends PathFilter&gt; filter);</code>
它默认过滤隐藏文件中以&rdquo;_&ldquo;和&rdquo;.&ldquo;开头的文件</p>
<pre class='brush:java'>  private static final PathFilter hiddenFileFilter = new PathFilter(){
      public boolean accept(Path p){
        String name = p.getName(); 
        return !name.startsWith("_") &amp;&amp; !name.startsWith("."); 
      }
    }; 
</pre><h4>FileInputFormat 类的输入分片</h4>
<p>FileInputFormat 类一般分割超过 HDFS 块大小的文件。通常分片与 HDFS 块大小一样，然后分片大小也可以改变的,下面展示了控制分片大小的属性：</p>
<p>待补。 TODO</p>
<pre class='brush:java'>FileInputFormat computeSplitSize(long goalSize, long minSize,long blockSize) {
    return Math.max(minSize, Math.min(goalSize, blockSize));
}
</pre><p>即：
<code>minimumSize &lt; blockSize &lt; maximumSize 分片的大小即为块大小。</code></p>
<p>重载 FileInputFormat 的 isSplitable() ＝false 可以避免 mapreduce 输入文件被分割。</p>
<h4>小文件与CombineFileInputFormat</h4>
<ol>
<li><p>CombineFileInputFormat 是针对小文件设计的，CombineFileInputFormat 会把多个文件打包到一个分片中，以便每个 mapper 可以处理更多的数据；减少大量小文件的另一种方法可以使用 SequenceFile 将这些小文件合并成一个或者多个大文件。</p>
</li>
<li><p>CombineFileInputFormat 不仅对于处理小文件实际上对于处理大文件也有好处，本质上，CombineFileInputFormat 使 map 操作中处理的数据量与 HDFS 中文件的块大小之间的耦合度降低了</p>
</li>
<li><p>CombineFileInputFormat 是一个抽象类，没有提供实体类，所以需要实现一个CombineFileInputFormat 具体
类和 getRecordReader() 方法(旧的接口是这个方法，新的接口InputFormat中则是createRecordReader())</p>
</li>
</ol>
<h4>把整个文件作为一条记录处理</h4>
<p>有时，mapper 需要访问问一个文件中的全部内容。即使不分割文件，仍然需要一个 RecordReader 来读取文件内容为 record 的值，下面给出实现这个功能的完整程序，详细解释见《Hadoop权威指南》。</p>
<h4>文本处理</h4>
<ol>
<li><p><strong>TextInputFileFormat</strong> 是默认的 InputFormat，每一行就是一个纪录</p>
</li>
<li><p>TextInputFileFormat 的 key 是 LongWritable 类型，存储该行在整个文件的偏移量，value 是每行的数据内容，不包括任何终止符(换行符和回车符)，它是Text类型.
如下例
On the top of the Crumpetty Tree</br>
</br>
The Quangle Wangle sat,</br>
But his face you could not see,</br>
On account of his Beaver Hat.</br>
每条记录表示以下key/value对</br>
(0, On the top of the Crumpetty Tree)</br>
(33, The Quangle Wangle sat,)</br>
(57, But his face you could not see,)</br>
(89, On account of his Beaver Hat.</p>
</li>
<li><p>输入分片与 HDFS 块之间的关系：TextInputFormat 每一条纪录就是一行，很可能某一行跨数据库存放。</p>
</li>
</ol>
<p><img src="http://zhaomingtai.u.qiniudn.com/Figure%207-3.%20Logical%20records%20and%20HDFS%20blocks%20for%20TextInputFormat.png" alt="image" /></p>
<ol>
<li><p><strong>KeyValueTextInputFormat</strong>。对下面的文本，KeyValueTextInputFormat 比较适合处理，其中可以通过
mapreduce.input.keyvaluelinerecordreader.key.value.separator 属性设置指定分隔符，默认
值为制表符，以下指定&rdquo;→&ldquo;为分隔符
</br>
line1→On the top of the Crumpetty Tree</br>
line2→The Quangle Wangle sat,</br>
line3→But his face you could not see,</br>
line4→On account of his Beaver Hat.</p>
</li>
<li><p><strong>NLineInputFormat</strong>。如果希望 mapper 收到固定行数的输入，需要使用 NLineInputFormat 作为 InputFormat 。与 TextInputFormat 一样，key是文件中行的字节偏移量，值是行本身。</p>
</li>
</ol>
<p>N 是每个 mapper 收到的输入行数，默认时 N=1，每个 mapper 会正好收到一行输入，mapreduce.input.lineinputformat.linespermap 属性控制 N 的值。以刚才的文本为例。
如果N=2，则每个输入分片包括两行。第一个 mapper 会收到前两行 key/value 对：</p>
<p>(0, On the top of the Crumpetty Tree)</br>
(33, The Quangle Wangle sat,)</br>
另一个mapper则收到：</br>
(57, But his face you could not see,)</br>
(89, On account of his Beaver Hat.)</br></p>
<h4>二进制输入</h4>
<p><strong>SequenceFileInputFormat</strong>
如果要用顺序文件数据作为 MapReduce 的输入，应用 SequenceFileInputFormat。key 和 value 顺序文件，所以要保证map输入的类型匹配</p>
<p>SequenceFileInputFormat 可以读 MapFile 和 SequenceFile，如果在处理顺序文件时遇到目录，SequenceFileInputFormat 类会认为值正在读 MapFile 数据文件。</p>
<p><strong>SequenceFileAsTextInputFormat</strong> 是 SequenceFileInputFormat 的变体。将顺序文件(其实就是SequenceFile)的 key 和 value 转成 Text 对象</p>
<p><strong>SequenceFileAsBinaryInputFormat</strong>是 SequenceFileInputFormat 的变体。将顺序文件的key和value作为二进制对象</p>
<h4>多种输入</h4>
<p>对于不同格式，不同表示的文本文件输出的处理，可以用 <strong>MultipleInputs</strong> 类里处理，它允许为每条输入路径指定 InputFormat 和 Mapper。</p>
<p>MultipleInputs 类有一个重载版本的 addInputPath()方法：</p>
<ul>
<li>旧api列举<pre class='brush:java'>public static void addInputPath(JobConf conf, Path path, Class&lt;? extends InputFormat&gt; inputFormatClass) 
</pre></li>
<li>新api列举<pre class='brush:java'>public static void addInputPath(Job job, Path path, Class&lt;? extends InputFormat&gt; inputFormatClass) 
</pre>在有多种输入格式只有一个mapper时候(调用Job的setMapperClass()方法)，这个方法会很有用。</li>
</ul>
<h4>DBInputFormat</h4>
<p>JDBC从关系数据库中读取数据的输入格式(参见权威指南)</p>
<h2>输出格式</h2>
<p>OutputFormat类的层次结构</p>
<p><img src="http://zhaomingtai.u.qiniudn.com/Figure%207-4.%20OutputFormat%20class%20hierarchy.png" alt="image" /></p>
<h3>文本输出</h3>
<p>默认输出格式是 <strong>TextOutputFormat</strong>，它本每条记录写成文本行，key/value 任意，这里 key和value 可以用制表符分割，用 mapreduce.output.textoutputformat.separator 书信可以改变制表符，与TextOutputFormat 对应的输入格式是 KeyValueTextInputFormat。</p>
<p>可以使用 NullWritable 来省略输出的 key 和 value。</p>
<h3>二进制输出</h3>
<ul>
<li><strong>SequenceFileOutputFormat</strong> 将它的输出写为一个顺序文件，因为它的格式紧凑，很容易被压缩，所以易于作为 MapReduce 的输入</li>
<li>把key/value对作为二进制格式写到一个 SequenceFile 容器中</li>
<li>MapFileOutputFormat 把 MapFile 作为输出，MapFile 中的 key 必需顺序添加，所以必须确保 reducer 输出的 key 已经排好序。</li>
</ul>
<h3>多个输出</h3>
<ul>
<li><p><strong>MultipleOutputFormat</strong> 类可以将数据写到多个文件中，这些文件名称源于输出的键和值。MultipleOutputFormat是个抽象类，它有两个子类：<strong>MultipleTextOutputFormat</strong> 和 <strong>MultipleSequenceFileOutputFormat</strong> 。它们是 TextOutputFormat 的和 SequenceOutputFormat 的多版本。</p>
</li>
<li><p><strong>MultipleOutputs</strong> 类
用于生成多个输出的库，可以为不同的输出产生不同的类型，无法控制输出的命名。它用于在原有输出基础上附加输出。输出是制定名称的。</p>
</li>
</ul>
<h4>MultipleOutputFormat和MultipleOutputs的区别</h4>
<p>这两个类库的功能几乎相同。MultipleOutputs 功能更齐全，但 MultipleOutputFormat 对 目录结构和文件命令更多de控制。</p>
<div  style="height:0px;border-bottom:1px dashed red"></div>
<table width="100%" border="1" cellpadding="3"  cellspacing="0" bordercolor="#eeeeee">
<tbody>
<tr>
    <td><em>特征   </em></td>
    <td><em>MultipleOutputFormat </em></td>
    <td><em>MultipleOutputs </em></td>
</tr>
<tr>
    <td>完全控制文件名和目录名 </td>
    <td>是 </td>
    <td>否 </td>
</tr>
<tr>
    <td>不同输出有不同的键和值类型 </td>
    <td>否 </td>
    <td>是 </td>
</tr>
<tr>
    <td>从同一作业的map和reduce使用 </td>
    <td>否 </td>
    <td>是 </td>
</tr>
<tr>
    <td>每个纪录多个输出 </td>
    <td>否 </td>
    <td>是 </td>
</tr>
<tr>
    <td>与任意OutputFormat一起使用 </td>
    <td>否，需要子类 </td>
    <td>是 </td>
</tr>
</tbody>
</table>
<h3>延时输出</h3>
<p>有些文件应用倾向于不创建空文件，此时就可以利用 LazyOutputFormat (Hadoop 0.21.0版本之后开始提供)，它是一个封装输出格式，可以保证指定分区第一条记录输出时才真正的创建文件，要使用它，用JobConf和相关输出格式作为参数来调用 setOutputFormatClass() 方法.</p>
<p>Streaming 和 Pigs 支持 -LazyOutput 选项来启用 LazyOutputFormat功能。</p>
<h3>数据库输出</h3>
<p>参见 关系数据和 HBase的输出格式。</p>
<h2>练习代码</h2>
<p>代码路径
https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/main/java/com/kangfoo/study/hadoop1/mp/typeformat </br>
使用 maven 打包之后用 hadoop jar 命令执行</br>
步骤同 Hadoop example jar 类</p>
<ol>
<li><p>使用 TextInputFormat 类型测试 wordcount
TestMapreduceInputFormat
上传一个文件</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -mkdir /test/input1
$ ./bin/hadoop fs -put ./wordcount.txt /test/input1
</pre><p>使用maven 打包 或者用eclipse hadoop 插件, 执行主函数时设置如下参数</br></p>
<pre class='brush:text'>hdfs://master11:9000/test/input1/wordcount.txt hdfs://master11:9000/numbers.seq hdfs://master11:9000/test/output5
</pre><p>没改过端口默认 namenode RPC 交互端口 8020 将上述的 9000 改成你自己的端口即可。</br>
部分日志</p>
<pre class='brush:shell'>## 准备运行程序和测试数据
lrwxrwxrwx.  1 hadoop hadoop      86 2月  17 21:02 study.hdfs-0.0.1-SNAPSHOT.jar -&gt; /home/hadoop/env/kangfoo.study/kangfoo/study.hdfs/target/study.hdfs-0.0.1-SNAPSHOT.jar
-rw-rw-r--.  1 hadoop hadoop    1983 2月  17 20:18 wordcount.txt
##执行
$ ./bin/hadoop jar study.hdfs-0.0.1-SNAPSHOT.jar TestMapreduceInputFormat /test/input1/wordcount.txt /test/output1
</pre></li>
<li><p>使用SequenceInputFormat类型测试wordcound
使用Hadoop权威指南中的示例创建 /numbers.seq 文件</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -text /numbers.seq
$ ./bin/hadoop jar study.hdfs-0.0.1-SNAPSHOT.jar TestMapreduceSequenceInputFormat /numbers.seq /test/output2
</pre></li>
<li><p>多文件输入</p>
<pre class='brush:shell'>$  ./bin/hadoop jar study.hdfs-0.0.1-SNAPSHOT.jar TestMapreduceMultipleInputs /test/input1/wordcount.txt /numbers.seq /test/output3
</pre></li>
</ol>
<h2>博客参考</h2>
<p><a href="http://www.taobaotest.com/categories/12/blogs">淘宝博客</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce 工作机制]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--gong-zuo-ji-zhi/"/>
    <updated>2014-03-03T22:17:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-mapreduce--gong-zuo-ji-zhi/</id>
    <content type="html"><![CDATA[<h2>工作流程</h2>
<ol>
<li>作业配置</li>
<li>作业提交</li>
<li>作业初始化</li>
<li>作业分配</li>
<li>作业执行</li>
<li>进度和状态更新</li>
<li>作业完成</li>
<li>错误处理</li>
<li>作业调度</li>
<li>shule（mapreduce核心）和sort</li>
</ol>
<h3>作业配置</h3>
<p>相对不难理解。 具体略。</p>
<h3>作业提交</h3>
<p><img src="http://zhaomingtai.u.qiniudn.com/mapredurce1.png" alt="image" /></p>
<p>首先熟悉上图，4个实例对象： client jvm、jobTracker、TaskTracker、SharedFileSystem</p>
<p>MapReduce 作业可以使用 JobClient.runJob(conf) 进行 job 的提交。如上图，这个执行过程主要包含了4个独立的实例。</p>
<ul>
<li>客户端。提交MapReduce作业。</li>
<li>jobtracker：协调作业的运行。jobtracker一个java应用程序。</li>
<li>tasktracker：运行作业划分后的任务。tasktracker一个java应用程序。</li>
<li>shared filesystem(分布式文件系统，如:HDFS)</li>
</ul>
<p>以下是Hadoop1.x 中旧版本的 MapReduce JobClient API. <strong>org.apache.hadoop.mapred.JobClient</strong></p>
<pre class='brush:java'>/** JobClient is the primary interface for the user-job to interact with the JobTracker. JobClient provides facilities to submit jobs, track their progress, access component-tasks' reports/logs, get the Map-Reduce cluster status information etc.
The job submission process involves:
Checking the input and output specifications of the job.
Computing the InputSplits for the job.
Setup the requisite accounting information for the DistributedCache of the job, if necessary.
Copying the job's jar and configuration to the map-reduce system directory on the distributed file-system.
Submitting the job to the JobTracker and optionally monitoring it's status.
Normally the user creates the application, describes various facets of the job via JobConf and then uses the JobClient to submit the job and monitor its progress. */ 
Here is an example on how to use JobClient:
     // Create a new JobConf
     JobConf job = new JobConf(new Configuration(), MyJob.class);
     // Specify various job-specific parameters     
     job.setJobName("myjob");
     job.setInputPath(new Path("in"));
     job.setOutputPath(new Path("out"));
     job.setMapperClass(MyJob.MyMapper.class);
     job.setReducerClass(MyJob.MyReducer.class);
     // Submit the job, then poll for progress until the job is complete
     JobClient.runJob(job);   
// JobClient.runJob(job) --&gt; JobClient. submitJob(job) --&gt;  submitJobInternal(job) 
</pre><p>新API放在 org.apache.hadoop.mapreduce.* 包下. 使用 Job 类代替 JobClient。又由job.waitForCompletion(true) 内部进行 JobClient.submitJobInternal() 封装。</p>
<p>新旧API请参考博文 <a href="http://www.cnblogs.com/beanmoon/archive/2012/12/06/2804905.html">Hadoop编程笔记（二）：Hadoop新旧编程API的区别</a></p>
<p>hadoop1.x 旧 API JobClient.runJob(job) 调用submitJob() 之后，便每秒轮询作业进度monitorAndPrintJob。并将其进度、执行结果信息打印到控制台上。</p>
<p>接着再看看 JobClient 的 submitJob() 方法的实现基本过程。上图步骤 2，3，4.</p>
<ol>
<li><p>向 jobtracker 请求一个新的 jobId. (<code>JobID jobId = jobSubmitClient.getNewJobId();</code> <code>void org.apache.hadoop.mapred.JobClient.init(JobConf conf) throws IOException</code> , 集群环境下是 RPC JobSubmissionProtocol 代理。本地环境使用 LocalJobRunner。</p>
</li>
<li><p>检查作业的相关的输出路径并提交 job 以及相关的 jar 到 job tracker, 相关的 libjar 通过distributedCache 传递给 jobtracker.</p>
<pre class='brush:java'>submitJobInternal(… …); 
// --&gt;
copyAndConfigureFiles(jobCopy, submitJobDir); 
// --&gt; 
copyAndConfigureFiles(job, jobSubmitDir, replication); 
… 
// --&gt; 
output.checkOutputSpecs(context);
</pre></li>
<li><p>计算作业的分片。将 SplitMetaInfo 信息写入 JobSplit。 Maptask 的个数 ＝ 输入的文件大小除以块的大小。</p>
<pre class='brush:java'>int maps = writeSplits(context, submitJobDir);
(JobConf)jobCopy.setNumMapTasks(maps);
// --&gt; 
maps = writeNewSplits(job, jobSubmitDir); 
// --&gt; （重写，要详细）
JobSplitWriter.createSplitFiles(jobSubmitDir, conf,
    jobSubmitDir.getFileSystem(conf), array); // List&lt;InputSplit&gt; splits = input.getSplits(job); 
// --&gt;
SplitMetaInfo[] info = writeNewSplits(conf, splits, out);
</pre></li>
<li><p>写JobConf信息到配置文件 job.xml。 <code>jobCopy.writeXml(out);</code></p>
</li>
<li><p>准备提交job。 RPC 通讯到 JobTracker 或者 LocalJobRunner.</p>
<pre class='brush:java'>jobSubmitClient.submitJob(jobId, submitJobDir.toString(), jobCopy.getCredentials());
</pre></li>
</ol>
<h3>作业初始化</h3>
<ol>
<li><p>当 JobTracker 接收到了 submitJob() 方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度。</p>
<pre class='brush:java'>submitJob(jobId, jobSubmitDir, null, ts, false);
// --&gt;
jobInfo = new JobInfo(jobId, new Text(ugi.getShortUserName()),
      new Path(jobSubmitDir));
</pre></li>
<li><p>作业调度器并对job进行初始化。初始化包括创建一个表示正在运行作业的对象——封装任务和纪录信息，以便跟踪任务的状态和进程（步骤5）。</p>
<pre class='brush:java'>job = new JobInProgress(this, this.conf, jobInfo, 0, ts);
// --&gt;
status = addJob(jobId, job);
// --&gt;
synchronized (jobs) {
  synchronized (taskScheduler) {
    jobs.put(job.getProfile().getJobID(), job);
    for (JobInProgressListener listener : jobInProgressListeners) {
      listener.jobAdded(job);
    }
  }
}
</pre></li>
<li><p>创建任务列表。在 JobInProgress的 initTask()方法中</p>
</li>
<li><p>从共享文件系统中获取 JobClient 已计算好的输入分片信息（步骤6）</p>
</li>
<li><p>创建 Map 任务和 Reduce 任务，为每个 MapTask 和 ReduceTask 生成 TaskProgress 对象。</p>
</li>
<li><p>创建的 reduce 任务的数量由 JobConf 的 mapred.reduce.task 属性决定，可用 setNumReduceTasks() 方法设置，然后调度器创建相应数量的要运行的 reduce 任务。任务被分配了 id。</p>
<pre class='brush:java'>JobInProgress initTasks() 
… …
TaskSplitMetaInfo[] splits = createSplits(jobId); // read input splits and create a map per a split
// --&gt;
allSplitMetaInfo[i] = new JobSplit.TaskSplitMetaInfo(splitIndex, 
      splitMetaInfo.getLocations(), 
      splitMetaInfo.getInputDataLength());
maps = new TaskInProgress[numMapTasks]; // 每个分片创建一个map任务
this.reduces = new TaskInProgress[numReduceTasks]; // 创建reduce任务
</pre></li>
</ol>
<h3>任务分配</h3>
<p>Tasktracker 和 JobTracker 通过心跳通信分配一个任务</p>
<ol>
<li><p>TaskTracker 定期发送心跳，告知 JobTracker, tasktracker 是否还存活，并充当两者之间的消息通道。</p>
</li>
<li><p>TaskTracker 主动向 JobTracker 询问是否有作业。若自己有空闲的 solt,就可在心跳阶段得到 JobTracker 发送过来的 Map 任务或 Reduce 任务。对于 map 任务和 task 任务，TaskTracker 有固定数量的任务槽，准确数量由 tasktracker 核的个数核内存的大小来确定。默认调度器在处理 reduce 任务槽之前，会填充满空闲的 map 任务槽，因此，如果 tasktracker 至少有一个空闲的 map 任务槽，tasktracker 会为它选择一个 map 任务，否则选择一个 reduce 任务。选择 map 任务时，jobTracker 会考虑数据本地化（任务运行在输入分片所在的节点），而 reduce 任务不考虑数据本地化。任务还可能是机架本地化。</p>
</li>
<li><p>TaskTracker 和 JobTracker heartbeat代码</p>
<pre class='brush:java'>TaskTracker.transmitHeartBeat()
// --&gt;
//
// Check if we should ask for a new Task
//
if (askForNewTask) {
  askForNewTask = enoughFreeSpace(localMinSpaceStart);
  long freeDiskSpace = getFreeSpace();
  long totVmem = getTotalVirtualMemoryOnTT();
  long totPmem = getTotalPhysicalMemoryOnTT();
  long availableVmem = getAvailableVirtualMemoryOnTT();
  long availablePmem = getAvailablePhysicalMemoryOnTT();
  long cumuCpuTime = getCumulativeCpuTimeOnTT();
  long cpuFreq = getCpuFrequencyOnTT();
  int numCpu = getNumProcessorsOnTT();
  float cpuUsage = getCpuUsageOnTT();
// --&gt;
// Xmit the heartbeat
HeartbeatResponse heartbeatResponse = jobClient.heartbeat(status, 
                                                          justStarted,
                                                          justInited,
                                                          askForNewTask, 
                                                          heartbeatResponseId);
注： InterTrackerProtocol jobClient RPC 到 JobTracker.heartbeat() 
JobTracker.heartbeat()
// --&gt;
// Process this heartbeat 
short newResponseId = (short)(responseId + 1);
status.setLastSeen(now);
if (!processHeartbeat(status, initialContact, now)) {
  if (prevHeartbeatResponse != null) {
    trackerToHeartbeatResponseMap.remove(trackerName);
  }
  return new HeartbeatResponse(newResponseId, 
               new TaskTrackerAction[] {new ReinitTrackerAction()});
}
</pre></li>
</ol>
<h3>任务执行</h3>
<p>tasktracker 执行任务大致步骤：</p>
<ol>
<li>被分配到一个任务后，从共享文件中把作业的jar复制到本地，并将程序执行需要的全部文件（配置信息、数据分片）复制到本地</li>
<li>为任务新建一个本地工作目录</li>
<li>内部类TaskRunner实例启动一个新的jvm运行任务</li>
</ol>
<p>Tasktracker.TaskRunner.startNewTask()代码</p>
<pre class='brush:java'>// --&gt;
RunningJob rjob = localizeJob(tip);
// --&gt;
launchTaskForJob(tip, new JobConf(rjob.getJobConf()), rjob); 
// --&gt;
tip.launchTask(rjob);
// --&gt;
setTaskRunner(task.createRunner(TaskTracker.this, this, rjob));
this.runner.start(); // MapTaskRunner 或者 ReduceTaskRunner
//
//startNewTask 方法完整代码：
void startNewTask(final TaskInProgress tip) throws InterruptedException {
    Thread launchThread = new Thread(new Runnable() {
      @Override
      public void run() {
        try {
          RunningJob rjob = localizeJob(tip);//初始化job工作目录
          tip.getTask().setJobFile(rjob.getLocalizedJobConf().toString());
          // Localization is done. Neither rjob.jobConf nor rjob.ugi can be null
          launchTaskForJob(tip, new JobConf(rjob.getJobConf()), rjob); // 启动taskrunner执行task
        } catch (Throwable e) {
          String msg = ("Error initializing " + tip.getTask().getTaskID() + 
                        ":\n" + StringUtils.stringifyException(e));
          LOG.warn(msg);
          tip.reportDiagnosticInfo(msg);
          try {
            tip.kill(true);
            tip.cleanup(false, true);
          } catch (IOException ie2) {
            LOG.info("Error cleaning up " + tip.getTask().getTaskID(), ie2);
          } catch (InterruptedException ie2) {
            LOG.info("Error cleaning up " + tip.getTask().getTaskID(), ie2);
          }
          if (e instanceof Error) {
            LOG.error("TaskLauncher error " + 
                StringUtils.stringifyException(e));
          }
        }
      }
    });
    launchThread.start();
  }
</pre><h3>进度和状态更新</h3>
<ol>
<li>状态包括：作业或认为的状态（成功，失败，运行中）、map 和 reduce 的进度、作业计数器的值、状态消息或描述</li>
<li>task 运行时，将自己的状态发送给 TaskTracker,由 TaskTracker 心跳机制向 JobTracker 汇报</li>
<li>状态进度由计数器实现</li>
</ol>
<p>如图：
<img src="http://zhaomingtai.u.qiniudn.com/updateStatusMapredurce.png" alt="image" /></p>
<h3>作业完成</h3>
<ol>
<li>jobtracker收到最后一个任务完成通知后，便把作业任务状态置为成功</li>
<li>同时jobtracker,tasktracker清理作业的工作状态</li>
</ol>
<h3>错误处理</h3>
<h4>task 失败</h4>
<ol>
<li>map 或者 reduce 任务中的用户代码运行异常，子 jvm 在进程退出之前向其父 tasktracker 发送报告, 并打印日志。tasktracker 会将此 task attempt 标记为 failed,释放一个任务槽 slot，以运行另一个任务。streaming 任务以非零退出代码，则标记为 failed.</li>
<li>子进程jvm突然退出（jvm bug）。tasktracker 注意到会将其标记为 failed。</li>
<li>任务挂起。tasktracker 注意到一段时间没有收到进度的更新，便将任务标记为 failed。此 jvm 子进程将被自动杀死。任务超时时间间隔通常为10分钟，使用 mapred.task.timeout 属性进行配置。以毫秒为单位。超时设置为0表示将关闭超时判定，长时间运行不会被标记为 failed，也不会释放任务槽。</li>
<li>tasktracker 通过心跳将子任务标记为失败后，自身计数器减一，以便向 jobtracker 申请新的任务</li>
<li>jobtracker 通过心跳知道一个 task attempt 失败之后，便重新调度该任务的执行（避开将失败的任务分配给执行失败的tasktracker）。默认执行失败尝试4次，若仍没有执行成功，整个作业就执行失败。</li>
</ol>
<h4>tasktracker 失败</h4>
<ol>
<li>一个 tasktracker 由于崩溃或者运行过于缓慢而失败，就会停止将 jobtracker 心跳。默认间隔可由 mapred.tasktracker.expriy.interval 设置，毫秒为单位。</li>
<li>同时 jobtracker 将从等待任务调度的 tasktracker 池将此 tasktracker 移除。jobtracker 重新安排此 tasktracker 上已运行并成功完成的 map 任务重新运行。</li>
<li>若 tasktracker 上面的失败任务数远远高于集群的平均失败数，tasktracker 将被列入黑名单。重启后失效。</li>
</ol>
<h4>jobtracker失败</h4>
<p>Hadoop jobtracker 失败是一个单点故障。作业失败。可在后续版本中启动多个 jobtracker,使用zookeeper协调控制（YARN）。</p>
<h3>作业调度</h3>
<ol>
<li>hadoop默认使用先进先出调度器（FIFO）
先遵循优先级优先，在按作业到来顺序调度。缺点：高优先级别的长时间运行的task占用资源，低级优先级，短作业得不到调度。</li>
<li>公平调度器（FairScheduler）
目标：让每个用户公平的共享集群的能力.默认情况下，每个用户都有自己的池。支持抢占，若一个池在特定的时间内未得到公平的资源分配共享，调度器将终止运行池中得到过多资源的任务，以便将任务槽让给资源不足的池。
详细文档参见：http://hadoop.apache.org/docs/r1.2.1/fair_scheduler.html</li>
<li>容量调度器（CapacityScheduler）
支持多队列，每个队列配置一定的资源，采用FIFO调度策略。对每个用户提交的作业所占的资源进行限定。
详细文档参见：http://hadoop.apache.org/docs/r1.2.1/capacity_scheduler.html</li>
</ol>
<h3>shuffle和sort</h3>
<p>mapreduce 执行排序，将 map 输出作为输入传递给 reduce 称为 shuffle。其确保每个 reduce 的输入都时按键排序。shuffle 是调优 mapreduce 重要的阶段。</p>
<p>mapreduce 的 shuffle 和排序如下图：
<img src="http://zhaomingtai.u.qiniudn.com/shuffle_sort.png" alt="image" /></p>
<h4>map端</h4>
<ol>
<li>map端并不是简单的将中间结果输出到磁盘。而是先用缓冲的方式写到内存，并预排序。</li>
<li>每个map任务都有一个环形缓冲区，用于存储任务的输出。默认100mb，由 io.sort.mb 设置。 io.sort.spill.percent 设置阀值，默认80%。</li>
<li>一旦内存缓冲区到达阀值，由一个后台线程将内存中内容 spill 到磁盘中。在写磁盘前，线程会根据数据最终要传送的 reducer 数目划分成相应的分区。每一个分区中，后台线程按键进行内排序，如果有一个 combiner 它会在排序后的输出上运行。</li>
<li>在任务完成之前，多个溢出写文件会被合并成一个已分区已排序的输出文件。最终成为 reduce 的输入文件。属性 io.sort.factor 控制一次最多能合并多少流（分区），默认10.</li>
<li>如果已指定 combiner,并且溢出写文件次数至少为3（min.num.spills.for.combiner 属性），则 combiner 就会在输出文件写到磁盘之前运行。目的时 map 输出更紧凑，写到磁盘上的数据更少。combiner 在输入上反复运行并不影响最终结果。</li>
<li>压缩 map 输出。写磁盘速度更快、节省磁盘空间、减少传给 reduce 数据量。默认不压缩。可使 mapred.compress.map.output=true 启用压缩，并指定压缩库, mapred.map.output.compression.codec。</li>
<li>reducer 通过HTTP方式获取输出文件的分区。由于文件分区的工作线程数量任务的 tracker.http.threads 属性控制。</li>
</ol>
<p>MapTask代码,内部类MapOutputBuffer.collect()方法在收集key/value到容器中,一旦满足预值，则开始溢出写文件由sortAndSpill() 执行。</p>
<pre class='brush:java'>// sufficient acct space
          kvfull = kvnext == kvstart;
          final boolean kvsoftlimit = ((kvnext &gt; kvend)
              ? kvnext - kvend &gt; softRecordLimit
              : kvend - kvnext &lt;= kvoffsets.length - softRecordLimit);
          if (kvstart == kvend &amp;&amp; kvsoftlimit) {
            LOG.info("Spilling map output: record full = " + kvsoftlimit);
            startSpill();
          }
// --&gt; startSpill();
 spillReady.signal(); //    private final Condition spillReady = spillLock.newCondition();
// --&gt; 溢出写文件主要由内部类 SpillThread（Thread） 执行
    try {
              spillLock.unlock();
              sortAndSpill(); // 排序并溢出
            } 
// --&gt; sortAndSpill()
 // create spill file
        final SpillRecord spillRec = new SpillRecord(partitions);
 // sorter = ReflectionUtils.newInstance(job.getClass("map.sort.class", QuickSort.class, IndexedSorter.class), job);
… …
 sorter.sort(MapOutputBuffer.this, kvstart, endPosition, reporter);
// --&gt;
 if (combinerRunner == null) {
… …
 // Note: we would like to avoid the combiner if we've fewer
              // than some threshold of records for a partition
              if (spstart != spindex) {
                combineCollector.setWriter(writer);
                RawKeyValueIterator kvIter =
                  new MRResultIterator(spstart, spindex);
                combinerRunner.combine(kvIter, combineCollector);
              }
}
</pre><h4>reduce 端</h4>
<ol>
<li>reduce 端 shuffle 过程分为三个阶段：复制 map 输出、排序合并、reduce 处理</li>
<li>reduce 可以接收多个 map 的输出。若 map 相当小，则会复制到 reduce tasktracker 的内存中（mapred.job.shuffle.input.buffer.pecent控制百分比）。一旦内存缓冲区达到阀值大小（由 mapped.iob.shuffle.merge.percent 决定）或者达到map输出阀值( mapred.inmem.merge.threshold 控制)，则合并后溢出写到磁盘</li>
<li>map任务在不同时间完成，tasktracker 通过心跳从 jobtracker 获取 map 输出位置。并开始复制 map 输出文件。</li>
<li>reduce 任务由少量复制线程，可并行复制 map 输出文件。由属性 mapred.reduce.parallel.copies 控制。</li>
<li>reduce 阶段不会等待所有输入合并成一个大文件后在进行处理，而是把部分合并的结果直接进行处理。</li>
</ol>
<p>ReduceTask源代码,run()方法</p>
<pre class='brush:java'>// --&gt; 3个阶段
 if (isMapOrReduce()) {
      copyPhase = getProgress().addPhase("copy");
      sortPhase  = getProgress().addPhase("sort");
      reducePhase = getProgress().addPhase("reduce");
    }
// --&gt; copy 阶段
if (!isLocal) {
      reduceCopier = new ReduceCopier(umbilical, job, reporter);
      if (!reduceCopier.fetchOutputs()) {
        if(reduceCopier.mergeThrowable instanceof FSError) {
          throw (FSError)reduceCopier.mergeThrowable;
        }
        throw new IOException("Task: " + getTaskID() + 
            " - The reduce copier failed", reduceCopier.mergeThrowable);
      }
    }
    copyPhase.complete();                         // copy is already complete
// --&gt; sort 阶段
setPhase(TaskStatus.Phase.SORT);
    statusUpdate(umbilical);
    final FileSystem rfs = FileSystem.getLocal(job).getRaw();
    RawKeyValueIterator rIter = isLocal
      ? Merger.merge(job, rfs, job.getMapOutputKeyClass(),
          job.getMapOutputValueClass(), codec, getMapFiles(rfs, true),
          !conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
          new Path(getTaskID().toString()), job.getOutputKeyComparator(),
          reporter, spilledRecordsCounter, null)
      : reduceCopier.createKVIterator(job, rfs, reporter);
    // free up the data structures
    mapOutputFilesOnDisk.clear();
    sortPhase.complete();                         // sort is complete
// --&gt; reduce 阶段
setPhase(TaskStatus.Phase.REDUCE); 
    statusUpdate(umbilical);
    Class keyClass = job.getMapOutputKeyClass();
    Class valueClass = job.getMapOutputValueClass();
    RawComparator comparator = job.getOutputValueGroupingComparator();
    if (useNewApi) {
      runNewReducer(job, umbilical, reporter, rIter, comparator, 
                    keyClass, valueClass);
    } else {
      runOldReducer(job, umbilical, reporter, rIter, comparator, 
                    keyClass, valueClass);
    }
// --&gt; done 执行结果
    done(umbilical, reporter);
</pre><h4>有关mapreduce shuffle和sort 原理、过程和调优</h4>
<p><a href="http://www.alidata.org/archives/1470">hadoop作业调优参数整理及原理</a>, <a href="http://langyu.iteye.com/blog/992916">MapReduce:详解Shuffle过程</a> 介绍的非常详尽。</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop RPC]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-rpc/"/>
    <updated>2014-03-02T23:33:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/03/hadoop-rpc/</id>
    <content type="html"><![CDATA[<p>Remote Procedure Call 远程方法调用。不需要了解网络细节，某一程序即可使用该协议请求来自网络内另一台及其程序的服务。它是一个 Client/Server 的结构,提供服务的一方称为Server，消费服务的一方称为Client。</p>
<p>Hadoop 底层的交互都是通过 rpc 进行的。例 如：datanode 和 namenode、tasktracker 和 jobtracker、secondary namenode 和 namenode 之间的通信都是通过 rpc 实现的。</p>
<p>TODO: 此文未写明了。明显需要画 4张图， rpc 原理图，Hadoop rpc 时序图， 客户端 流程图，服端流程图。最好帖几个包图＋ 类图（组件图）。待完善。</p>
<p><strong>要实现远程过程调用，需要有3要素</strong>：
1、server 必须发布服务
2、在 client 和 server 两端都需要有模块来处理协议和连接
3、server 发布的服务，需要将接口给到 client</p>
<h2>Hadoop RPC</h2>
<ol>
<li>序列化层。 Client 与 Server  端通讯传递的信息采用实现自 Writable 类型</li>
<li>函数调用层。 Hadoop RPC 通过动态代理和 java 反射实现函数调用</li>
<li>网络传输层。Hadoop RPC 采用 TCP/IP socket 机制</li>
<li>服务器框架层。Hadoop RPC 采用 java NIO 事件驱动模型提高 RPC Server 吞吐量</li>
</ol>
<p>TODO 缺个 RPC 图</p>
<p>Hadoop RPC 源代码主要在org.apache.hadoop.ipc包下。org.apache.hadoop.ipc.RPC 内部包含5个内部类。</p>
<ul>
<li>Invocation ：用于封装方法名和参数，作为数据传输层，相当于VO（Value Object）。</li>
<li>ClientCache ：用于存储client对象，用 socket factory 作为 hash key,存储结构为 hashMap <SocketFactory, Client>。</li>
<li>Invoker ：是动态代理中的调用实现类，继承了 java.lang.reflect.InvocationHandler。</li>
<li>Server ：是ipc.Server的实现类。</li>
<li>VersionMismatch : 协议版本。</li>
</ul>
<h3>从客户端开始进行通讯源代码分析</h3>
<p>org.apache.hadoop.ipc.Client 有5个内部类</p>
<ul>
<li>Call: A call waiting for a value.</li>
<li>Connection: Thread that reads responses and notifies callers.  Each connection owns a socket connected to a remote address.  Calls are multiplexed through this socket: responses may be delivered out of order.</li>
<li>ConnectionId: This class holds the address and the user ticket. The client connections to servers are uniquely identified by <remoteAddress, protocol, ticket></li>
<li>ParallelCall: Call implementation used for parallel calls.</li>
<li>ParallelResults: Result collector for parallel calls.</li>
</ul>
<p><strong>客户端和服务端建立连接的大致执行过程为</strong>：</p>
<ol>
<li><p>在 Object org.apache.hadoop.ipc.RPC.Invoker.invoke(Object proxy, Method method, Object[] args) 方法中调用</br>
client.call(new Invocation(method, args), remoteId);</p>
</li>
<li><p>上述的 new Invocation(method, args) 是 org.apache.hadoop.ipc.RPC 的内部类，它包含被调用的方法名称及其参数。此处主要是设置方法和参数。 client 为 org.apache.hadoop.ipc.Client 的实例对象。</p>
</li>
<li><p>org.apache.hadoop.ipc.Client.call() 方法的具体源代码。在call()方法中 getConnection()内部获取一个 org.apache.hadoop.ipc.Client.Connection 对象并启动 io 流 setupIOstreams()。</p>
<pre class='brush:java'>Writable org.apache.hadoop.ipc.Client.call(Writable param, ConnectionId remoteId) throwsInterruptedException, IOException {
Call call = new Call(param); //A call waiting for a value.   
// Get a connection from the pool, or create a new one and add it to the
// pool.  Connections to a given ConnectionId are reused. 
Connection connection = getConnection(remoteId, call);// 主要在 org.apache.hadoop.net 包下。
connection.sendParam(call); //客户端发送数据过程
boolean interrupted = false;
synchronized (call) {
   while (!call.done) {
    try {
      call.wait();                           // wait for the result
    } catch (InterruptedException ie) {
      // save the fact that we were interrupted
      interrupted = true;
    }
  }
… …
}
}
// Get a connection from the pool, or create a new one and add it to the
// pool.  Connections to a given ConnectionId are reused. 
private Connection getConnection(ConnectionId remoteId,
                               Call call)
                               throws IOException, InterruptedException {
if (!running.get()) {
  // the client is stopped
  throw new IOException("The client is stopped");
}
Connection connection;
// we could avoid this allocation for each RPC by having a  
// connectionsId object and with set() method. We need to manage the
// refs for keys in HashMap properly. For now its ok.
do {
  synchronized (connections) {
    connection = connections.get(remoteId);
    if (connection == null) {
      connection = new Connection(remoteId);
      connections.put(remoteId, connection);
    }
  }
} while (!connection.addCall(call)); 
//we don't invoke the method below inside "synchronized (connections)"
//block above. The reason for that is if the server happens to be slow,
//it will take longer to establish a connection and that will slow the
//entire system down.
connection.setupIOstreams(); // 向服务段发送一个 header 并等待结果
return connection;
}
</pre></li>
<li><p>setupIOstreams() 方法。</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Client.Connection.setupIOstreams() throws InterruptedException {
// Connect to the server and set up the I/O streams. It then sends
// a header to the server and starts
// the connection thread that waits for responses.
while (true) {
      setupConnection();//  建立连接
      InputStream inStream = NetUtils.getInputStream(socket); // 输入
      OutputStream outStream = NetUtils.getOutputStream(socket); // 输出
      writeRpcHeader(outStream);
      }
… … 
// update last activity time
  touch();
// start the receiver thread after the socket connection has been set up            start(); 
}        
</pre></li>
<li><p>启动org.apache.hadoop.ipc.Client.Connection
客户端获取服务器端放回数据过程</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Client.Connection.run()
while (waitForWork()) {//wait here for work - read or close connection
    receiveResponse();
  }
</pre></li>
</ol>
<h3>ipc.Server源码分析</h3>
<p>ipc.Server 有6个内部类：</p>
<ul>
<li>Call ：用于存储客户端发来的请求</li>
<li>Listener ： 监听类，用于监听客户端发来的请求，同时Listener内部还有一个静态类，Listener.Reader，当监听器监听到用户请求，便让Reader读取用户请求。</li>
<li>ExceptionsHandler: 异常管理</li>
<li>Responder ：响应RPC请求类，请求处理完毕，由Responder发送给请求客户端。</li>
<li>Connection ：连接类，真正的客户端请求读取逻辑在这个类中。</li>
<li>Handler ：请求处理类，会循环阻塞读取callQueue中的call对象，并对其进行操作。</li>
</ul>
<p>大致过程为：</p>
<ol>
<li><p>Namenode的初始化时，RPC的server对象是通过ipc.RPC类的getServer()方法获得的。</p>
<pre class='brush:java'>void org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(Configuration conf) throwsIOException
// create rpc server
InetSocketAddress dnSocketAddr = getServiceRpcServerAddress(conf);
if (dnSocketAddr != null) {
  int serviceHandlerCount =
    conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,
                DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);
  this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(), 
      dnSocketAddr.getPort(), serviceHandlerCount,
      false, conf, namesystem.getDelegationTokenSecretManager());
  this.serviceRPCAddress = this.serviceRpcServer.getListenerAddress();
  setRpcServiceServerAddress(conf);
}
… …
this.server.start();  //start RPC server  
</pre></li>
<li><p>启动 server</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.start()
// Starts the service.  Must be called before any calls will be handled.
public synchronized void start() {
responder.start();
listener.start();
handlers = new Handler[handlerCount];
for (int i = 0; i &lt; handlerCount; i++) {
  handlers[i] = new Handler(i);
  handlers[i].start(); //处理call
}
}
</pre></li>
<li><p>Server处理请求, server 同样使用非阻塞 nio 以提高吞吐量</p>
<pre class='brush:java'>org.apache.hadoop.ipc.Server.Listener.Listener(Server) throws IOException
public Listener() throws IOException {
  address = new InetSocketAddress(bindAddress, port);
  // Create a new server socket and set to non blocking mode
  acceptChannel = ServerSocketChannel.open();
  acceptChannel.configureBlocking(false);
… … }     
</pre></li>
<li><p>真正建立连接</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.Listener.doAccept(SelectionKey key) throws IOException,OutOfMemoryError
</pre><p>Reader 读数据接收请求</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.Listener.doRead(SelectionKey key) throws InterruptedException
try {
    count = c.readAndProcess();
  } catch (InterruptedException ieo) {
    LOG.info(getName() + ": readAndProcess caught InterruptedException", ieo);
    throw ieo;
  }
</pre><pre class='brush:java'>int org.apache.hadoop.ipc.Server.Connection.readAndProcess() throws IOException,InterruptedException
if (!rpcHeaderRead) {
      //Every connection is expected to send the header.
      if (rpcHeaderBuffer == null) {
        rpcHeaderBuffer = ByteBuffer.allocate(2);
      }
      count = channelRead(channel, rpcHeaderBuffer);
      if (count &lt; 0 || rpcHeaderBuffer.remaining() &gt; 0) {
        return count;
      }
      int version = rpcHeaderBuffer.get(0);
… … 
processOneRpc(data.array()); // 数据处理
</pre></li>
<li><p>下面贴出Server.Connection类中的processOneRpc()方法和processData()方法的源码。</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.Connection.processOneRpc(byte[] buf) throws IOException,InterruptedException
private void processOneRpc(byte[] buf) throws IOException,
    InterruptedException {
  if (headerRead) {
    processData(buf);
  } else {
    processHeader(buf);
    headerRead = true;
    if (!authorizeConnection()) {
      throw new AccessControlException("Connection from " + this
          + " for protocol " + header.getProtocol()
          + " is unauthorized for user " + user);
    }
  }
}
</pre></li>
<li><p>处理call</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.Handler.run()
while (running) {
    try {
      final Call call = callQueue.take(); // pop the queue; maybe blocked here
      … … 
      CurCall.set(call);
      try {
        // Make the call as the user via Subject.doAs, thus associating
        // the call with the Subject
        if (call.connection.user == null) {
          value = call(call.connection.protocol, call.param, 
                       call.timestamp);
        } else {
… …}
</pre></li>
<li><p>返回请求</p>
</li>
</ol>
<p>下面贴出Server.Responder类中的doRespond()方法源码：</p>
<pre class='brush:java'>void org.apache.hadoop.ipc.Server.Responder.doRespond(Call call) throws IOException
    //
    // Enqueue a response from the application.
    //
    void doRespond(Call call) throws IOException {
      synchronized (call.connection.responseQueue) {
        call.connection.responseQueue.addLast(call);
        if (call.connection.responseQueue.size() == 1) {
          processResponse(call.connection.responseQueue, true);
        }
      }
    }
</pre><p>补充：
notify()让因wait()进入阻塞队列里的线程（blocked状态）变为runnable，然后发出notify()动作的线程继续执行完，待其完成后，进行调度时，调用wait()的线程可能会被再次调度而进入running状态。</p>
<p>参考资源：</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop I/O]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-io-1/"/>
    <updated>2014-02-26T23:50:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-io-1/</id>
    <content type="html"><![CDATA[<p>HDFS 对网络IO, 磁盘IO 的操作是比较复杂且开销还比较高的。Hadoop 在设计中使用了内部的原子操作、压缩、随机读写、流式存储、数据完整性校验、序列化、基于文件的数据结构等方面进行 IO 操作。</p>
<h2>数据完整性</h2>
<p>保证数据在传输过程中不损坏，常见的保证数据完整性采用的技术</p>
<ul>
<li>奇偶校验技术</li>
<li>ECC 内存纠错校验技术</li>
<li>CRC-32 循环冗余校验技术</li>
</ul>
<h3>HDFS的数据完整性</h3>
<p>HDFS 会对写入的所有数据计算校验和，并在读取数据时验证校验和。它针对每个由 io.bytes.per.checksum(默认512字节，开销低于1%)指定字节数据技术校验和。</p>
<p>DataNode 负责在验证收到的数据后存储数据及其校验和。从客户端和其它数据节点复制过来的数据。客户端写入数据并且将它发送到一个数据节点管线中，在管线的最后一个数据节点验证校验和。</p>
<p>客户端读取 DataNode 上的数据时，也会验证校验和。将其与 DataNode 上存储的校验和进行对比。每个 DataNode 维护一个连续的校验和验证日志，因此它知道每个数据块最后验证的时间。</p>
<p>每个 DataNode 还会在后台线程运行一个 DataBlockScanner（数据块检测程序），定期验证存储在数据节点上的所有块，以解决物理存储媒介上位损坏问题。</p>
<p>HDFS 通过复制完整的数据复本来修复损坏的数据块，进而得到一个新的、完好无损的复本。基本思路：如果客户端读取数据块时检测到错误，就向 NameNode 汇报已损坏的数据块及它试图从名称节点中要读取的 DataNode,并抛出 ChecksumException。 NameNode 将这个已损坏的数据块复本标记为已损坏，并不直接与 datanode 联系，或尝试将这个个复本复制到另一个 datanode。之后，namennode 安排这个数据块的一个复本复制到另一个 datanode。 至此，数据块复制因子恢复到期望水平。此后，并将已损坏的数据块复本删除。</p>
<h3>LocalFileSystem</h3>
<p>Hadoop的 LocalFileSystem 执行客户端校验。意味着，在写一个名filename的文件时，文件系统的客户端以透明的方式创建一个隐藏.filename.crc。在同一个文件夹下，包含每个文件块的校验和。<br  /></p>
<p>禁用校验和,使用底层文件系统原生支持校验和。这里通过 RawLocalFileSystem  来替代 LocalFileSystem 完成。要在一个应用中全局使用，只需要设置 fs.file.impl值为 org.apache.hadoop.fs.RawLocalFileSystem 来重新 map 执行文件的 URL。或者只想对某些读取禁用校验和校验。例：</p>
<pre class='brush:java'>Configuration conf = ...
FileSystem fs = new RawLocalFileSystem();
fs.initialize(null, conf)；
</pre><h3>ChecksumFileSystem</h3>
<p>LocalFileSystem 继承自 ChecksumFileSystem(校验和文件系统)，ChecksumFileSystem 继承自 FileSystem。ChecksumFileSystem 可以很容易的添加校验和功能到其他文件系统中。</p>
<h2>压缩</h2>
<p>将文件压缩有两大好处</p>
<ul>
<li>减少存储文件所需要的磁盘空间</li>
<li>加速数据在网络和磁盘上的传输</li>
</ul>
<h3>编译native-hadoop</h3>
<p>参见 《Native-hadoop 编译》</p>
<h3>压缩算法</h3>
<p>所有的压缩算法都需要权衡时间/空间比.压缩和解压缩速度越快，节省空间越少。gizp压缩空间/时间性能比较适中。bzip2比gzip更高效，但数度更慢; lzo 压缩速度比gzip比较快，但是压缩效率稍微低一点。</p>
<!--| Tables        | Are           | Cool  |
| ------------- |:-------------:| -----:|
| col 3 is      | right-aligned | $1600 |
| col 2 is      | centered      |   $12 |
| zebra stripes | are neat      |    $1 |-->
<p><strong>Hadoop支持的压缩格式</strong></p>
<div  style="height:0px;border-bottom:1px dashed red"></div>
<table width="100%" border="1" cellpadding="6"  cellspacing="0" bordercolor="#eeeeee">
<tbody>
<tr>
    <td><em>压缩格式 </em></td>
    <td><em>工具 </em></td>
    <td><em>算法 </em></td>
    <td><em>文件扩展名 </em></td>
    <td><em>多文件 </em></td>
    <td><em>可切分 </em></td>
</tr>
<tr>
    <td>DEFLATE </td>
    <td>无 </td>
    <td>DEFLATE </td>
    <td>.deflate </td>
    <td>否 </td>
    <td>否 </td>
</tr>
<tr>
    <td>Gzip</td>
    <td>gzip</td>
    <td>DEFLATE</td>
    <td>.gz</td>
    <td>否</td>
    <td>否</td>
</tr>
<tr>
    <td>bzip2</td>
    <td>bzip2</td>
    <td>bzip2</td>
    <td>.bz</td>
    <td>否</td>
    <td>是</td>
</tr>
<tr>
    <td>LZO</td>
    <td>Lzop</td>
    <td>LZO</td>
    <td>.lzo</td>
    <td>否</td>
    <td>否</td>
</tr>
</tbody>
</table>
<div  style="height:0px;border-bottom:1px dashed red"></div>
<p></br>
<strong>编码/解码</strong>
用以执行压缩解压算法，是否有java／原生实现</p>
<div  style="height:0px;border-bottom:1px dashed red"></div>
<table width="100%" border="1" cellpadding="4"  cellspacing="0" bordercolor="#eeeeee">
<tbody>
<tr>
    <td><em>压缩格式 </em></td>
    <td><em>codec</em></td>
    <td><em>java实现</em></td>
    <td><em>原生实现</em></td>
</tr>
<tr>
    <td>DEFLATE</td>
    <td>org.apache.hadoop.io.compress.DefaultCodec</td>
    <td>是</td>
    <td>是</td>
</tr>
<tr>
    <td>gzip</td>
    <td>org.apache.hadoop.io.compress.GzipCodec</td>
    <td>是</td>
    <td>是</td></tr>
<tr>
    <td>bzip2</td>
    <td>org.apache.hadoop.io.compress.Bzip2Codec</td>
    <td>是</td>
    <td>否</td>
</tr>
<tr>
    <td>LZO</td>
    <td>com.hadoop.compression.lzo.LzopCodec</td>
    <td>否</td>
    <td>是</td>
</tr>
</tbody>
</table>
<div  style="height:0px;border-bottom:1px dashed red"></div>
<p></br>
<strong>压缩算法相关的 API</strong></p>
<p>使用 CompressionCodecFactory.getCodec()方法来推断 CompressionCodec 具体实现。由 CompressionCodec 接口的实现对流进行进行压缩与解压缩。CodecPool 提供了重复利用压缩和解压缩的对象的机制。</p>
<p>… … 画个类图。## TOTO</p>
<p>NativeCodeLoader 加载 native-hadoop library
若想使用 snappycode 首先加载 snappy.so,再判断加载 native hadoop&ndash;>hadoop.so。native hadoop 中包含了 java 中申明的native 方法，由 native 方法去调用第三方的 natvie library。<a href="http://hadoop.apache.org/docs/r1.2.1/native_libraries.html">native_libraries官方参考文档</a></p>
<p>在 Hadoop core-site.xml 配置文件中可以设置是否使用本地库,默认以启用。</p>
<pre class='brush:xml'>&lt;property&gt;
  &lt;name&gt;hadoop.native.lib&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;Should native hadoop libraries, if present, be used.&lt;/description&gt;
&lt;/property&gt;
</pre><h3>编写使用压缩的测试程序</h3>
<ol>
<li>首先下载并编译 snappy,zlib</li>
<li>编写 java 代码 <a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/CompressionTest.java">CompressionTest.java</a>, <a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/DeCompressionTest.java">DeCompressionTest.java</a>。 程序是由 maven test 进行。</li>
<li>执行<pre class='brush:shell'>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hadoop/env/hadoop/lib/native/Linux-amd64-64:/usr/local/lib
$ mvn test -Dtest=com.kangfoo.study.hadoop1.io.CompressionTest
$ mvn test -Dtest=com.kangfoo.study.hadoop1.io.DeCompressionTest
## 结果：
rw-rw-r--. 1 hadoop hadoop 531859 7月  23 2013 releasenotes.html
-rw-rw-r--. 1 hadoop hadoop 140903 1月  22 15:13 releasenotes.html.deflate
-rw-rw-r--. 1 hadoop hadoop 531859 1月  22 15:22 releasenotes.html.deflate.decp
-rw-rw-r--. 1 hadoop hadoop 140915 1月  22 15:13 releasenotes.html.gz
-rw-rw-r--. 1 hadoop hadoop 531859 1月  22 15:22 releasenotes.html.gz.decp
-rw-rw-r--. 1 hadoop hadoop 224661 1月  22 15:13 releasenotes.html.snappy
-rw-rw-r--. 1 hadoop hadoop 531859 1月  22 15:22 releasenotes.html.snappy.decp
## 日志：
Running com.kangfoo.study.hadoop1.io.CompressionTest
2014-01-22 15:13:31,312 WARN  snappy.LoadSnappy (LoadSnappy.java:&lt;clinit&gt;(36)) - Snappy native library is available
2014-01-22 15:13:31,357 INFO  util.NativeCodeLoader (NativeCodeLoader.java:&lt;clinit&gt;(43)) - Loaded the native-hadoop library
2014-01-22 15:13:31,357 INFO  snappy.LoadSnappy (LoadSnappy.java:&lt;clinit&gt;(44)) - Snappy native library loaded
2014-01-22 15:13:31,617 INFO  zlib.ZlibFactory (ZlibFactory.java:&lt;clinit&gt;(47)) - Successfully loaded &amp; initialized native-zlib library
</pre></li>
</ol>
<h3>启用压缩</h3>
<p>出于性能考虑，使用原生的压缩库要比同时提供 java 实现的开销更小。可以修改 Hadoop core-site.xml 配置文件 io.compression.codecs 以启用压缩，前提是必须安装好对应的原生压缩库依赖，并配置正确的 Codec。</p>
<ul>
<li>属性名: io.compression.codecs</li>
<li>默认值:org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.ompress.Bzip2Codec</li>
</ul>
<h3>压缩与输入分割</h3>
<p>考虑如何压缩将由 MapReduce 处理的数据时，是否支持<strong>分割</strong>很重要。</p>
<p><strong>案例假设</strong>，一个gzip压缩的文件的为1GB。HDFS 将其分为16块(64mb 块大小)，其中每一数据块最为一个 map 任务输入。那么在 map 任务中，每一个分块是无法独立工作的（ gzip 是使用的 DEFLATE 算法，它将数据存储在一系列的压缩块中。无法实现从数据流的任意位置读取数据，那么这些分块必须全部读取并与整个数据流进行同步才能从任意位置进行读取数据）。这样就失去了本地化的优势。一个 map 要处理其他15个分块的数据，而大多数据并不存储在当前 map 节点上。Map的任务数越少，作业的粒度就较大，运行的时间可能会更长。</p>
<p>具体应该选择哪种压缩形式，还要经过测试，才可以决定。大文件选择支持分割的压缩形式，目前只有 bzip2 支持分片，但没有原生库的实现。或者使用 SequenceFile, MapFile 数据格式进行小文件的合并再存储，这样可以满足分片。</p>
<h3>在 MapReduce 中使用压缩</h3>
<p>如果文件是压缩过的，那么在被 MapReduce 读取时，它们会被解压，根据文件扩展名选择对应的解码器。可参考 MapReduce 块压缩相关知识。</p>
<p><strong>压缩 MapReduce 的作业输出</strong></p>
<ol>
<li>在作业配置中将 mapred.output.compress 属性设置为 true</li>
<li>将 mapred.output.compression.codec 属性设置为自己需要使用的压缩解码/编码器的类名。</li>
</ol>
<p><strong>代码示例</strong></p>
<pre class='brush:java'>conf.setBoolean(“mapred.output.compress”,true)
Conf.setClass(“mapred.output.compression.codec”,GizpCodec.class,CompressionCodec.class);
</pre><p><strong>对 Map 任务输出结果的压缩</strong></p>
<p>压缩 Map 作业的中间结果以减少网络传输。</p>
<p>Map输出压缩属性</br>
属性名称: mapred.compress.map.output </br>
类型: boolean </br>
默认值: false </br>
描述: 对 map 任务输出是否进行压缩 </br>
</br>
属性名称: mapred.map.output.compression.codec <br>
类型: Class <br>
默认值: org.apache.hadoop.io.compress.DefaultCodec <br>
描述: map 输出所用的压缩 codec <br></p>
<p><strong>代码示例</strong></p>
<pre class='brush:java'>conf.setCompressMapOutput(true);
conf.setMapOutputCompressorClass(GzipCodec.classs)
</pre><p></br></p>
<h2>序列化和反序列化</h2>
<p>什么是Hadoop的序列化? 序列化，将结构化对象转换为字节流，以便于在网络传输和磁盘存储的过程。反序列化，将字节流转化为结构化的对象的逆过程。可用于进程间的通讯和永久存储，数据拷贝</p>
<p>序列化特点：</p>
<ul>
<li>紧凑：可充分利用网络带宽（哈夫曼编码）</li>
<li>快速：尽量减少序列化和反序列化的开销</li>
<li>可扩展：通讯协议升级向下兼容</li>
<li>互操作：支持不同语言间的通讯</li>
</ul>
<p>Hadoop1.x 仅满足了紧凑和快速两个特性。
java 自身提供的序列化并不精简。Java Serializaiton 是序列化图对象的通讯机制，它有序列化和反序列化的开销。
java 序列化比较复杂，不能很精简的控制对象的读写。连接／延迟／缓冲。java 序列化不能满足： 精简，快速，可扩展，可互操作。</p>
<p>Hadoop1.x 使用 Writable 实现自己的序列化格式。它结构紧凑，快速。但难以用 java 以外的语言进行扩展。</p>
<h3>Writable 接口</h3>
<p>Writeable 接口定义了2个方法：</p>
<pre class='brush:java'>void write(DataOutput out) throws IOException; // 将其状态写入二进制格式的 DataOutput 流；
void readFields(DataInput in) throws IOException; // 从二进制格式的 DataInput 流读取其状态
</pre><p>画个类图 ## TODO</p>
<pre class='brush:java'>writable
writableComparable(interface WritableComparable&lt;T&gt; extends Writable, Comparable&lt;T&gt; )
comparator(int compare(T o1, T o2);)
comparable(public int compareTo(T o);)
rawcomparator(public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);)
writablecomparator(ReflectionUtils.newInstance(keyClass, null);)
</pre><p><strong>Writable 类的层次结构</strong>
<img src="http://zhaomingtai.u.qiniudn.com/writable.png" alt="image" /></p>
<p><strong>部分类型列举</strong>：</p>
<ul>
<li>NullWritable 是一种特殊的Writable类型，单例的, 序列化的长度是零。可以做占位符。</li>
<li>Text 是针对UTF-8序列化的Writable类。一般可等价于 java.lang.String 的 Writable。Text是可变的。</li>
<li>BytesWritable 是一个对二进制的封装，序列化为一个格式为一个用于制定后面数据字节数的整数域（4字节），后跟字节本身。它是可变的。如：<pre class='brush:java'>BytesWritable b = new BytesWritable(new byte[]{2,5,127}); // 3个长度
byte[] bytes = serialize(b);
assertThat(StringUtils.byteToHexString(bytes), is("0000000302057f"));
</pre></li>
<li>ObjectWritable 适用于java基本类型（String，enum，Writable，null或者这些类型组成的数组）的一个封装。</li>
<li>Writable集合。<strong>ArrayWritable和TwoDArrayWritable</strong>针对于数组和二维数组，它们中所有的元素必须是同一个类的实例。<strong>MapWritable和SortedMapWritable</strong>是针对于 Map 和 SorMap。</li>
</ul>
<p><strong>自定义Writable</strong>
•实现WritableComparable
•实现</p>
<pre class='brush:java'>write(); // 将对象转换为字节流并写入到输出流 out 中
readFields(); // 从输入流 in 中读取字节流并反序列化为对象
compareTo()方法。 // 将 this 对像与对象 O 比较
</pre><p><strong>示例程序代码</strong>：</p>
<ul>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/WritableComparableTest.java">WritableComparableTest</a></li>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/WritableComparableTest2.java">WritableComparableTest2</a></li>
</ul>
<h3>序列化框架</h3>
<ul>
<li>apache avro 旨在解决Hadoop中Writable类型的不足：缺乏语言的可移植性。</li>
<li>apache thrift 可伸缩的跨语言, 提供了 PRC 实现层。</li>
<li>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，很适合做数据存储或 RPC 数据交换格式。它可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。</li>
</ul>
<p><strong>参考</strong>：</p>
<ul>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-gpb/">Google Protocol Buffer 的使用和原理</a></li>
<li>Hadoop 2.2.0 中使用protocol buffer。源文件路径：hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/proto。</li>
</ul>
<h2>基于文件的数据结构</h2>
<p>使用 SequenceFile, MapFile 主要解决的问题是：支持分片的数据压缩格式的比较有限，对于某些应用而言，需要处理的数据格式来存储自己的格式，MapRedurce 需要更高级的容器。</p>
<h3>SequenceFile</h3>
<ol>
<li>文件是以二进制键／值对形式存储的平面文件</li>
<li>可以作为小文件的容器，它将小文件包装起来，以获取更高效率的存储和处理</li>
<li>存储在 SequenceFile 中的 key/valu e并不一定是 Writable 类型</li>
<li>可使用 append()方法在文件末位附加 key/value 对</li>
</ol>
<p><strong>好处</strong>：</p>
<ol>
<li>支持纪录或者块压缩</li>
<li>支持splittable， 可作为mapreduce输入分片</li>
<li>修改简单（har是不可以修改的）</li>
</ol>
<p><strong>SequenceFile 压缩</strong></p>
<p>SequenceFile 文件格式内部结构与是否启用压缩有关。启用压缩又分两类：纪录压缩；数据块压缩。</p>
<ol>
<li><p>无压缩。 默认是不启用压缩，则每个纪录就是它的纪录长度（字节数）、键长度、键和值组成。长度字段为4字节的整数。</p>
</li>
<li><p>纪录压缩。其格式与无压缩情况相同，不同在于纪录压缩的值需要通过文件头中定义的压缩codec进行压缩。键不压缩。</br>
无压缩和纪录压缩的示意图：
<img src="http://zhaomingtai.u.qiniudn.com/SequenceFile.png" alt="image" /></p>
</li>
<li><p>块压缩。一次压缩多条纪录，比单条纪录压缩效率高。可以不断的向数据块中压缩纪录，直到字节数不小于io.seqfile.compress.blocksize属性中设置的字节数。默认1MB.每个新的块的开始处都需要插入同步标识。数据块的格式如下：首先是一个指示数据块中字节数的字段；紧跟是4个压缩字段（键长度、键、值长度、值）。块压缩示意图如下：
<img src="http://zhaomingtai.u.qiniudn.com/block_compression.png" alt="image" /></p>
</li>
</ol>
<p><strong>实例程序代码</strong></p>
<ul>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/SequenceFileReadDemo.java">SequenceFileReadDemo</a></li>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/SequenceFileWriteDemo.java">SequenceFileWriteDemo</a></li>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/SequenceFileTest.java">SequenceFileTest（纪录压缩，块压缩）</a></li>
</ul>
<p><strong>运行结果</strong></p>
<pre class='brush:shell'>## 查看 sequence file
$ ./bin/hadoop fs -text /numbers.seq
## 排序
$ ./bin/hadoop jar ./hadoop-examples-1.2.1.jar sort -r 1 -inFormat org.apache.hadoop.mapred.SequenceFileInputFormat -outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat -outKey org.apache.hadoop.io.IntWritable -outValue org.apache.hadoop.io.Text /numbers.seq sorted
## 查看排序后的结果（原键降序排列为从1到100升序排列）
$ ./bin/hadoop fs -text /user/hadoop/sorted/part-00000
</pre><p><strong>博客参考</strong></p>
<h3>MapFile</h3>
<p>MapFile 是已经排序的 SequenceFile，可以视为 java.util.Map 持久化形式。它已加入了搜索键的索引，可以根据 key 进行查找。它的键必须是 WritableComparable 类型的实例，值必须是 Writable 类型的实例，而 SequenceFile 无此要求。使用 MapFile.fix() 方法进行索引重建，把 SequenceFile 转换为 MapFile。</p>
<p>MapFile java 源代码</p>
<pre class='brush:java'>org.apache.hadoop.io.MapFile.Writer{ 
// 类的内部结构（MapFile是已经排序的SequenceFile）：
private SequenceFile.Writer data;
private SequenceFile.Writer index;
… … 
}
org.apache.hadoop.io.MapFile.Reader{
// 二分法查找。一次磁盘寻址 ＋ 一次最多顺序128（默认值等于每128下一个索引）个条目顺序扫瞄
public synchronized Writable get(WritableComparable key, Writable val){
… … 
}
</pre><p><strong>实例程序代码</strong></p>
<ul>
<li><a href="https://github.com/kangfoo/hadoop1.study/blob/08db06fd75cc5b10a07a457cb499a98d6c69da29/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/io/MapFileTest.java">MapFileTest</a></li>
</ul>
<p><strong>运行结果</strong></p>
<pre class='brush:shell'>$ ./bin/hadoop fs -text /numbers.map/data 
$ ./bin/hadoop fs -text /numbers.map/index
</pre><p><strong>SequenceFile合并为MapFile</strong></p>
<ol>
<li>新建SequenceFile文件<pre class='brush:shell'>$ ./bin/hadoop jar ./hadoop-examples-1.2.1.jar sort -r 1 -inFormat org.apache.hadoop.mapred.SequenceFileInputFormat -outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat -outKey org.apache.hadoop.io.IntWritable -outValue org.apache.hadoop.io.Text /numbers.seq /numbers2.map
</pre></li>
<li>重命名文件夹<pre class='brush:shell'>$ ./bin/hadoop fs -mv /numbers2.map/part-00000 /numbers2.map/data
</pre></li>
<li>运行测试用例</li>
<li>验证结果<pre class='brush:shell'>Created MapFile hdfs://master11:9000/numbers2.map with 100 entries
rw-r--r--   2 hadoop      supergroup       4005 2014-02-09 20:06 /numbers2.map/data
-rw-r--r--   3 kangfoo-mac supergroup        136 2014-02-09 20:13 /numbers2.map/index
</pre></li>
</ol>
<!--TODO : 此页面过大，内容太多，主题不突出，需要拆分为 3 个。-->
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop pipes 编译]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-c-pipes--bian-yi/"/>
    <updated>2014-02-26T01:01:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-c-pipes--bian-yi/</id>
    <content type="html"><![CDATA[<p>我在编译 Hadoop Pipes 的时候，出现了些小问题。主要是我没有安装 openssl-devel。本以为安装 openssl 就差不多了，可这个就是问题的根源, 我现在是自己动手编译 pipes, 而 Hadoop 的 pipes 编译需要 openssl 的依赖，那么在编译的时候最好还是将 openssl-devel 开发支持的依赖补上比较省事。在解决问题的时发现网上向我一样的同学还是有的。在此我就贴下我编译时的部分日志。</p>
<ol>
<li><p>在Hadoop 根目录下执行</p>
<pre class='brush:shell'>ant -Dcompile.c++=yes examples
##错误
[exec] checking for HMAC_Init in -lssl... no
BUILD FAILED
/home/hadoop/env/hadoop-1.2.1/build.xml:2164: exec returned: 255
… … 
./configure: line 5234: exit: please: numeric argument required
##具体日志：
… … 
 [exec] configure: error: Cannot find libssl.so ## 没有 libssl.so
 [exec] /home/hadoop/env/hadoop-1.2.1/src/c++/pipes/configure: line 5234: exit: please: numeric argument required
 [exec] /home/hadoop/env/hadoop-1.2.1/src/c++/pipes/configure: line 5234: exit: please: numeric argument required
 [exec] checking for HMAC_Init in -lssl... no 
</pre></li>
<li><p>检查 ssl</p>
<pre class='brush:shell'>$ yum info openssl
$ ll /usr/lib64/libssl*
-rwxr-xr-x. 1 root root 221568 2月  23 2013 /usr/lib64/libssl3.so
lrwxrwxrwx. 1 root root     16 12月  8 18:14 /usr/lib64/libssl.so.10 -&gt; libssl.so.1.0.1e
-rwxr-xr-x. 1 root root 436984 12月  4 04:21 /usr/lib64/libssl.so.1.0.1e
## 缺个 libssl.so 的文件, 于是添加软链接：
sudo ln -s /usr/lib64/libssl.so.1.0.1e /usr/lib64/libssl.so
</pre></li>
<li><p>切换目录到 pipes 下再次编译</p>
<pre class='brush:shell'>$cd /home/hadoop/env/hadoop/src/c++/pipes
执行
$ make distclean
$ ./configure 
[hadoop@master11 pipes]$ ./configure 
checking for a BSD-compatible install... /usr/bin/install -c
… … 
checking whether it is safe to define __EXTENSIONS__... yes
checking for special C compiler options needed for large files... no
checking for _FILE_OFFSET_BITS value needed for large files... no
checking pthread.h usability... yes
checking pthread.h presence... yes
checking for pthread.h... yes
checking for pthread_create in -lpthread... yes
checking for HMAC_Init in -lssl... no
configure: error: Cannot find libssl.so ## 还是没找到
./configure: line 5234: exit: please: numeric argument required
./configure: line 5234: exit: please: numeric argument required
</pre></li>
<li><p>安装openssl-devel, <code>sudo yum install openssl-devel</code></p>
</li>
<li><p>再切换到Hadoop根目录下执行</p>
<pre class='brush:shell'>ant -Dcompile.c++=yes examples
##搞定，编译通过
compile-examples:
[javac] /home/hadoop/env/hadoop-1.2.1/build.xml:742: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
[javac] Compiling 24 source files to /home/hadoop/env/hadoop-1.2.1/build/examples
[javac] 警告: [options] 未与 -source 1.6 一起设置引导类路径
[javac] 注: /home/hadoop/env/hadoop-1.2.1/src/examples/org/apache/hadoop/examples/MultiFileWordCount.java使用或覆盖了已过时的 API。
[javac] 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
[javac] 1 个警告
examples:
  [jar] Building jar: /home/hadoop/env/hadoop-1.2.1/build/hadoop-examples-1.2.2-SNAPSHOT.jar
BUILD SUCCESSFUL
Total time: 1 minute 11 seconds
</pre></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[native-hadoop 编译]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/nativehadoop--bian-yi/"/>
    <updated>2014-02-26T00:57:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/nativehadoop--bian-yi/</id>
    <content type="html"><![CDATA[<p>对我来讲编译 native hadoop 并不是很顺利。现将问题纪录在案。</p>
<h3>主要问题</h3>
<ol>
<li>ivy 联网获取资源并不稳定</li>
<li>hadoop-1.2.1/build.xml:62: Execute failed: java.io.IOException: Cannot run program “autoreconf” (in directory “/home/userxxx/hadoop/hadoop-1.2.1/src/native”): java.io.IOException: error=2, No such file or directory</li>
<li>[exec] configure: error: Zlib headers were not found&hellip; native-hadoop library needs zlib to build. Please install the requisite zlib development package.</li>
<li>多次编译失败之后要记得执行 make distclean 清理一下。</li>
<li>编译完 ant compile-native 之后，启动 hadoop 使用 http 访问 /dfshealth.jsp /jobtracker.jsp HTTP ERROR 404</li>
<li>在 Linux 平台下编译 native hadoop 是不可以的，目前。错误：/hadoop-1.2.1/build.xml:694: exec returned: 1</li>
</ol>
<h3>解决方案</h3>
<ol>
<li>第一个问题只能多次尝试。</li>
<li>第二，第三个问题主要是是没有安装 zlib。顺便请保证 gcc c++, autoconf, automake, libtool, openssl,openssl-devel 也安装。安装 zlib 请参考 http://www.zlib.net/ 。</li>
<li>第四个问题就是 基本的 make 三部曲的步骤。</li>
<li>第五个问题原因是在 build native 库的同时,生成了 webapps 目录(在当前的 target 这个目录是个基本的结构，没有任何 jsp 等资源，404找不到很正常)。那么当我们编译过build之后，hadoop启动时又指向了这个目录，就导致这个错误。我们就可以直接将这个 build 文件夹删除了或者改脚本。问题搞定了。</li>
<li>第六个问题，援引官方<pre>
Supported Platforms
The native hadoop library is supported on *nix platforms only. The library does not to work with Cygwin or the Mac OS X platform.
</pre>
那就老实点用 *nix platforms，就没事了。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Java JNI练习]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/java-jni--lian-xi/"/>
    <updated>2014-02-26T00:55:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/java-jni--lian-xi/</id>
    <content type="html"><![CDATA[<p>在 Hadoop 中大量使用了 JNI 技术以提高性能并利用其他语言已有的成熟算法简化开发难度。如压缩算法、pipes 等。那么在具体学习 native hadoop, hadoop io 前先简单复习下相关知识。在此我主要是参见了 Oracle 官方网站 + IBM developerworks + csdn 论坛 写了个简单的 Hello World 程序。</p>
<p>此处主要是将我参考的资源进行了列举，已备具体深入学习参考。</p>
<ul>
<li><a href="http://docs.oracle.com/javase/6/docs/technotes/guides/jni/">JAVA JNI oracle 官方文档</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/java/j-jni/">IBM 使用 Java Native Interface 的最佳实践</a></li>
<li><a href="http://blog.csdn.net/yangguo_2011/article/details/17379297#1536434-youdao-1-7173-6552c87762fd890a5b54a8bfc2e2f443">JNI hello world</a></li>
</ul>
<p><strong>JNI 编程主要步骤</strong></p>
<ol>
<li>编写一个.java</li>
<li>javac *.java</li>
<li>javah -jni className -> *.h</li>
<li>创建一个.so/.dll 动态链接库文件</li>
</ol>
<p><strong>编程注意事项</strong></p>
<ol>
<li>不要直接使用从java里面传递过来的value.(在java里面的对象在本地调用前可能被jvm析构函数了)</li>
<li>一旦不使用某对象或者变量，要去ReleaseXXX()。</li>
<li>不要在 native code 里面去申请内存</li>
<li>使用 javap -s 查看java 签名</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HDFS API 练习使用]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hdfs-api--lian-xi-shi-yong/"/>
    <updated>2014-02-26T00:53:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hdfs-api--lian-xi-shi-yong/</id>
    <content type="html"><![CDATA[<p>经过前几页博客的知识巩固，现在开始使用 Hadoop API 不是什么难事。此处不重点讲述。参考 Hadoop API 利用 FileSystem 实例对象操作 FSDataInputStream/FSDataOutputStream 基本不是问题。</p>
<h4>HDFS API入门级别的使用</h4>
<ol>
<li><p>获取 FileSystem 对象</br>
get(Configuration conf)</br>
Configuration 对象封装了客户端或者服务器端的 conf/core-site.xml 配置</b></p>
</li>
<li><p>通过 FileSystem 对象进行文件操作</br>
读数据：open()获取FSDataInputStream(它支持随机访问),</br>
写数据：create()获取FSDataOutputStream</br></p>
</li>
</ol>
<p>参考代码：<a href="https://github.com/kangfoo/hadoop1.study/blob/master/kangfoo/study.hdfs/src/test/java/com/kangfoo/study/hadoop1/htfs/HDFSTest.java">HDFSTest.java</a></p>
<p>代码中主要利用 FileSystem 对象进行文件的 读、写、重命名、删除、文件信息获取等操作。</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop 机架感知]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-ji-jia-gan-zhi/"/>
    <updated>2014-02-26T00:52:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-ji-jia-gan-zhi/</id>
    <content type="html"><![CDATA[<p>HDFS 和 Map/Reduce 的组件是能够感知机架的。</p>
<p>NameNode 和 JobTracker 通过调用管理员配置模块中的 API resolve 来获取集群里每个 slave 的机架id。该 API 将 slave 的 DNS 名称（或者IP地址）转换成机架id。使用哪个模块是通过配置项 topology.node.switch.mapping.impl 来指定的。模块的默认实现会调用 topology.script.file.name 配置项指定的一个的脚本/命令。 如果 topology.script.file.name 未被设置，对于所有传入的IP地址，模块会返回 /default-rack 作为机架 id。</p>
<p>在 Map/Reduce 部分还有一个额外的配置项 mapred.cache.task.levels ，该参数决定 cache 的级数（在网络拓扑中）。例如，如果默认值是2，会建立两级的 cache—— 一级针对主机（主机 -> 任务的映射）另一级针对机架（机架 -> 任务的映射）。</p>
<p>我目前没有模拟环境先纪录个参考博客 <a href="http://www.cnblogs.com/ggjucheng/archive/2013/01/03/2843015.html">机架感知</a> 以备后用。</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[模拟使用 SecondaryNameNode 恢复 NameNode]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/shi-yong-secondenamenode-hui-fu-namenode/"/>
    <updated>2014-02-26T00:50:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/shi-yong-secondenamenode-hui-fu-namenode/</id>
    <content type="html"><![CDATA[<h2>SecondaryNameNode</h2>
<p>在试验前先了解下什么是 SecondaryNameNode、它的原理、检查点等知识点。再依次从开始配置 SecondaryNameNode 检查点、准备测试环境、模拟正常的 NameNode 故障，并手动启动 NameNode 并从 SecondaryNameNode 中恢复 fsimage。</p>
<p><strong>注</strong>：此试验思路主要借鉴于<a href="http://new.osforce.cn/?mu=20140227220525KZol8ENMYdFQ6SjMveU26nEZ">开源力量</a>的<a href="http://new.osforce.cn/course/101?mc101=20140301233857au7XG16o9ukfev1pmFCOfv2s">LouisT 老师 Hadoop Development 课程</a>中的SecondaryNameNode章节。</p>
<h3>作用</h3>
<p>主要是为了解决namenode单点故障。不是 namenode 的备份。它周期性的合并 fsimage ( namenode 的镜像)和 editslog（或者 edits——所有对 fsimage 镜像文件操作的步骤）,并推送给 namenode 以辅助恢复namenode。</p>
<p>SecondaryNameNode 定期合并 fsimage 和 edits 日志，将 edits 日志文件大小控制在一个限度下。因为内存需求和 NameNode 在一个数量级上，所以通常 SecondaryNameNode 和 NameNode 运行在不同的机器上。SecondaryNameNode 通过bin/start-dfs.sh 在 conf/masters 中指定的节点上启动。</p>
<p>在hadoop 2.x 中它的作用可以被两个节点替换：checkpoint node(于 SecondaryNameNode 作用相同), backup node( namenode 的完全备份)</p>
<h3>原理（具体可参见《Hadoop权威指南》第10章 管理Hadoop）</h3>
<p>edits 文件纪录了所有对 fsimage 镜像文件的写操作的步骤。文件系统客户端执行写操作时，这些操作首先会被记录到 edits 文件中。Nodename 在内存中维护文件系统的元数据；当 edits 被修改时，相关元数据也同步更新。内存中的元数据可支持客户端的读请求。</p>
<p>在每次执行写操作之后，且在向客户端发送成功代码之前，edits 编辑日志都需要更新和同步。当 namedone 向多个目录写数据时，只有在所有写操作执行完毕之后方可返回成功代码，以保证任何操作都不会因为机器故障而丢失。</p>
<p>fsimage 文件是文件系统元数据的一个永久检查点。它包含文件系统中的所有目录和文件 inode 的序列化谢谢。每个 inode 都是一个文件或目录的元数据的内部描述方式。对于文件来说，包含的信息有“复制级别”、修改时间、访问时间、访问许可、块大小、组成一个文件的块等；对于目录来说，包含的信息有修改时间、访问许可和配额元数据等信息。</p>
<p>fsimage 是一个大型文件，频繁执行写操作，会使系统运行极慢。并非每一写操作都会更新到 fsimage 文件。
SecondaryNameNode 辅助 namenode，为 namenode 内存中的文件系统元数据创建检查点，并最终合并并更新 fsimage 镜像和减小 edits 文件。</p>
<h3>SecondaryNameNode 的检查点</h3>
<p>SecondaryNameNode 进程启动是由两个配置参数控制的。</p>
<ul>
<li>fs.checkpoint.period，指定连续两次检查点的最大时间间隔， 默认值是1小时。</li>
<li>fs.checkpoint.size 定义了 edits 日志文件的最大值，一旦超过这个值会导致强制执行检查点（即使没到检查点的最大时间间隔）。默认值是64MB。</li>
</ul>
<h3>SecondaryNameNode 检查点的具体步骤</h3>
<p><img src="http://zhaomingtai.u.qiniudn.com/snn.png" alt="image" /></p>
<ol>
<li>SecondaryNameNode 请求主 namenode 停止使用 edits 文件，暂时将新的操作记录到 edits.new 文件中；</li>
<li>SecondaryNameNode 以 http get 复制 主 namenode 中的 fsimage, edits 文件；</li>
<li>SecondaryNameNode 将 fsimage 载入到内存，并逐一执行 edits 文件中的操作，创建新的fsimage.ckpt 文件；</li>
<li>SecondaryNameNode 以 http post 方式将新的fsimage.ckp 复制到主namenode.</li>
<li>主 namenode 将 fsimage 文件替换为 fsimage.ckpt，同时将 edits.new 文件重命名为 edits。并更新 fstime 文件来记录下次检查点时间。</li>
</ol>
<p>SecondaryNameNode 保存最新检查点的目录与 NameNode 的目录结构相同。 所以 NameNode 可以在需要的时候读取 SecondaryNameNode上的检查点镜像。</p>
<h3>模拟 NameNode 故障以从 SecondaryNameNode 恢复</h3>
<p>场景假设：如果NameNode上除了最新的检查点以外，所有的其他的历史镜像和 edits 文件都丢失了，NameNode 可以引入这个最新的检查点以恢复。具体模拟步骤如下：</p>
<ol>
<li>在配置参数 dfs.name.dir 指定的位置建立一个空文件夹；</li>
<li>把检查点目录的位置赋值给配置参数 fs.checkpoint.dir；</li>
<li>启动NameNode，并加上-importCheckpoint。</li>
</ol>
<p>NameNode 会从 fs.checkpoint.dir 目录读取检查点，并把它保存在 dfs.name.dir 目录下。 如果 dfs.name.dir 目录下有合法的镜像文件，NameNode 会启动失败。 NameNode 会检查fs.checkpoint.dir 目录下镜像文件的一致性，但是不会去改动它。</p>
<h3>试验从 SecondaryNameNode 中备份恢复 NameNode</h3>
<p><strong>注意</strong>：此步骤执行并不能将原的数据文件系统从物理磁盘上移除，同样也不能在新格式化的 namenode 中查看旧的文件系统文件。请确定无误再试验。</p>
<h4>试验知识准备</h4>
<p>命令的使用方法请参考 <a href="http://hadoop.apache.org/docs/r1.2.1/commands_manual.html#secondarynamenode">SecondaryNameNode 命令</a>。在试验前，可先了解些 hadoop 的默认配置
<a href="http://hadoop.apache.org/docs/r1.1.2/core-default.html">core-site.xml-default</a>,
<a href="http://hadoop.apache.org/docs/r1.1.2/hdfs-default.html">hdfs-site.xml-default</a>,
<a href="http://hadoop.apache.org/docs/r1.1.2/mapred-default.html">mapred-site.xml-default</a></p>
<p>SecondarynameNode 相关属性描述：</p>
<pre>
属性：fs.checkpoint.dir     
值：${hadoop.tmp.dir}/dfs/namesecondary
描述：Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.
fs.checkpoint.edits.dir

属性：${fs.checkpoint.dir}     
值：Determines where on the local filesystem the DFS secondary name node should 
描述：store the temporary edits to merge. If this is a comma-delimited list of directoires then teh edits is replicated in all of the directoires for redundancy. Default value is same as fs.checkpoint.dir

属性：fs.checkpoint.period  
值：3600   
描述：The number of seconds between two periodic checkpoints.

属性：fs.checkpoint.size    
值：67108864   
描述：The size of the current edit log (in bytes) that triggers a periodic checkpoint even if the fs.checkpoint.period hasn't expired.
</pre>
<h4>试验环境配置</h4>
<ol>
<li><p>首先修改 core-site.xml 文件中的配置，主要是调小了 checkpoint 的周期并指定 SSN 的目录。</p>
<pre class='brush:xml'>&lt;property&gt;
&lt;name&gt;fs.checkpoint.period&lt;/name&gt;
&lt;value&gt;120&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.checkpoint.dir&lt;/name&gt;
&lt;value&gt;/home/${user.name}/env/data/snn&lt;/value&gt;
&lt;/property&gt;
</pre><p><code>vi hdfs-site.xml</code> 查看 NameNode 数据文件存储路径</p>
<pre class='brush:xml'>&lt;property&gt;
 &lt;name&gt;dfs.name.dir&lt;/name&gt;
 &lt;value&gt;/home/${user.name}/env/data/name&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;dfs.data.dir&lt;/name&gt;
 &lt;value&gt;/home/${user.name}/env/data/data&lt;/value&gt;
&lt;/property&gt;
</pre></li>
<li><p>再次，format namenode 。 <code>./bin/hadoop namenode -format</code>。查看当前的 master namenode namespaceID <code>cat ./name/current/VERSION</code></p>
<pre class='brush:shell'>#Tue Jan 21 15:14:40 CST 2014
namespaceID=1816120670 ## 文件系统的唯一标识符
cTime=0 ## namenode的创建时间，刚格式化为0，升级之后为时间戳
storageType=NAME_NODE ## 存储类型
layoutVersion=-41 ## 负的整数。描述了hdfs永久性数据结构的版本。与Hadoop的版本无关。与升级有关。
</pre></li>
<li><p>查看 datanode 下的version。<code>cat data/current/VERSION</code></p>
<pre class='brush:shell'>#Tue Jan 21 09:51:42 CST 2014
namespaceID=80003531
storageID=DS-949100596-192.168.56.12-50010-1387691685116
cTime=0
storageType=DATA_NODE
layoutVersion=-41
</pre><p>若 namespaceID 不相同，请将 datanode 中的id修改为 namenode 相同的 namespaceID。
同样的步骤修改其他的 datanode.
若是第一次format可以跳过此步骤。此步骤要注意避免如下错误(Incompatible namespaceID)：</p>
<pre class='brush:shell'>2014-01-21 15:07:54,890 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible namespaceIDs in /home/hadoop/env/data/data: namenode namespaceID = 2020545490; datanode namespaceID = 80003531
</pre></li>
</ol>
<h4>查看 NameNode 试验前正常环境状况</h4>
<ol>
<li><p>启动hdfs./bin/start-dfs.sh</p>
</li>
<li><p>jps 检查所有的进程(当前NameNode进程正常)</p>
<pre class='brush:shell'>5832 SecondaryNameNode
6293 Jps
5681 NameNode
2212 DataNode
2198 DataNode
</pre></li>
<li><p>创建测试数据</p>
<pre class='brush:shell'>$ ./bin/hadoop fs -mkdir /test
$ ./bin/hadoop fs -lsr /
drwxr-xr-x   - hadoop supergroup          0 2014-01-21 15:21 /test
[hadoop@master11 hadoop]$ ./bin/hadoop fs -put ivy.xml /test
[hadoop@master11 hadoop]$ ./bin/hadoop fs -lsr /
drwxr-xr-x   - hadoop supergroup          0 2014-01-21 15:22 /test
-rw-r--r--   2 hadoop supergroup      10525 2014-01-21 15:22 /test/ivy.xml
</pre></li>
<li><p>查看 SecondaryNameNode 文件目录</p>
<pre class='brush:shell'>watch ls ./data/snn/ 
current
image
in_use.l
</pre></li>
<li><p>namenode 对应日志</p>
<pre class='brush:shell'>2014-01-21 15:54:02,654 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.56.11
2014-01-21 15:54:02,654 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0
2014-01-21 15:54:02,655 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: closing edit log: position=4, editlog=/home/hadoop/env/data/name/current/edits
2014-01-21 15:54:02,655 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: close success: truncate to 4, editlog=/home/hadoop/env/data/name/current/edits
2014-01-21 15:54:02,778 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://0.0.0.0:50090/getimage?getimage=1
2014-01-21 15:54:02,781 INFO org.apache.hadoop.hdfs.server.namenode.GetImageServlet: Downloaded new fsimage with checksum: 4a75545e83f108e21ef321fb0066ede4
2014-01-21 15:54:02,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll FSImage from 192.168.56.11
2014-01-21 15:54:02,781 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 1 SyncTimes(ms): 56
2014-01-21 15:54:02,784 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: closing edit log: position=4, editlog=/home/hadoop/env/data/name/current/edits.new
2014-01-21 15:54:02,784 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: close success: truncate to 4, editlog=/home/hadoop/env/data/name/current/edits.new
</pre></li>
<li><p>namenode 文件目录</p>
<pre class='brush:shell'>$ cd name/
$ tree
.
├── current
│   ├── edits
│   ├── fsimage
│   ├── fstime
│   └── VERSION
├── image
│   └── fsimage
├── in_use.lock
└── previous.checkpoint
├── edits
├── fsimage
├── fstime
└── VERSION
</pre></li>
</ol>
<h4>模拟 NameNode 故障</h4>
<ol>
<li>人为的杀掉 namenode 进程<pre class='brush:shell'>kill -9 6690 ## 6690 NameNode
删除 namenode 元数据
$ rm -rf ./data/name/*
删除 Secondary NameNode in_use.lock 文件 
$ rm -rf ./snn/in_use.lock
</pre></li>
</ol>
<h4>从 SecondaryNameNode 中恢复 NameNode</h4>
<ol>
<li><p>启动以 importCheckpoint 方式启动 NameNode。<code>$ ./bin/hadoop namenode -importCheckpoint</code></p>
</li>
<li><p>验证是否恢复成功</p>
<pre class='brush:shell'>## HDFS 文件系统正常
$ ./bin/hadoop fsck /
The filesystem under path '/' is HEALTHY
$ ./bin/hadoop fs -lsr /
## 元文件信息已恢复
drwxr-xr-x   - hadoop supergroup          0 2014-01-21 15:22 /test
-rw-r--r--   2 hadoop supergroup      10525 2014-01-21 15:22 /test/ivy.xml
$ tree
.
├── data
├── name(已恢复)
│   ├── current
│   │   ├── edits
│   │   ├── fsimage
│   │   ├── fstime
│   │   └── VERSION
│   ├── image
│   │   └── fsimage
│   ├── in_use.lock
│   └── previous.checkpoint
│       ├── edits
│       ├── fsimage
│       ├── fstime
│       └── VERSION
├── snn
│   ├── current
│   │   ├── edits
│   │   ├── fsimage
│   │   ├── fstime
│   │   └── VERSION
│   ├── image
│   │   └── fsimage
│   └── in_use.lock
└── tmp
9 directories, 16 files
</pre></li>
<li><p>查看恢复日志信息(截取部分信息)</p>
<pre class='brush:shell'>## copy fsimage
14/01/21 16:57:52 INFO common.Storage: Storage directory /home/hadoop/env/data/name is not formatted.
14/01/21 16:57:52 INFO common.Storage: Formatting ...
14/01/21 16:57:52 INFO common.Storage: Start loading image file /home/hadoop/env/data/snn/current/fsimage
14/01/21 16:57:52 INFO common.Storage: Number of files = 3
14/01/21 16:57:52 INFO common.Storage: Number of files under construction = 0
14/01/21 16:57:52 INFO common.Storage: Image file /home/hadoop/env/data/snn/current/fsimage of size 274 bytes loaded in 0 seconds.
##copy edits
4/01/21 16:57:52 INFO namenode.FSEditLog: Start loading edits file /home/hadoop/env/data/snn/current/edits
14/01/21 16:57:52 INFO namenode.FSEditLog: EOF of /home/hadoop/env/data/snn/current/edits, reached end of edit log Number of transactions found: 0.  Bytes read: 4
14/01/21 16:57:52 INFO namenode.FSEditLog: Start checking end of edit log (/home/hadoop/env/data/snn/current/edits) ...
14/01/21 16:57:52 INFO namenode.FSEditLog: Checked the bytes after the end of edit log (/home/hadoop/env/data/snn/current/edits):
14/01/21 16:57:52 INFO namenode.FSEditLog:   Padding position  = -1 (-1 means padding not found)
14/01/21 16:57:52 INFO namenode.FSEditLog:   Edit log length   = 4
14/01/21 16:57:52 INFO namenode.FSEditLog:   Read length       = 4
14/01/21 16:57:52 INFO namenode.FSEditLog:   Corruption length = 0
14/01/21 16:57:52 INFO namenode.FSEditLog:   Toleration length = 0 (= dfs.namenode.edits.toleration.length)
14/01/21 16:57:52 INFO namenode.FSEditLog: Summary: |---------- Read=4 ----------|-- Corrupt=0 --|-- Pad=0 --|
14/01/21 16:57:52 INFO namenode.FSEditLog: Edits file /home/hadoop/env/data/snn/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/21 16:57:52 INFO common.Storage: Image file /home/hadoop/env/data/name/current/fsimage of size 274 bytes saved in 0 seconds.
14/01/21 16:57:54 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/home/hadoop/env/data/name/current/edits
14/01/21 16:57:54 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/home/hadoop/env/data/name/current/edits
14/01/21 16:57:54 INFO namenode.FSEditLog: Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 
## 恢复 fsimage
14/01/21 16:57:54 INFO namenode.FSNamesystem: Finished loading FSImage in 1971 msecs
14/01/21 16:57:54 INFO hdfs.StateChange: STATE* Safe mode ON
... ...
14/01/21 16:58:26 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 15 msec
14/01/21 16:58:26 INFO hdfs.StateChange: STATE* Leaving safe mode after 33 secs
## 离开安全模式 Safe mode is OFF
14/01/21 16:58:26 INFO hdfs.StateChange: STATE* Safe mode is OFF
14/01/21 16:58:26 INFO hdfs.StateChange: STATE* Network topology has 1 racks and 2 datanodes
14/01/21 16:58:26 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
</pre></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop 分布式文件系统]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-hdfs-1/"/>
    <updated>2014-02-25T22:46:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hadoop-hdfs-1/</id>
    <content type="html"><![CDATA[<p>管理网络跨多台计算机存储的文件系统称为分布式文件系统。当数据的大小超过单台物理计算机存储能力，就需要对它进行分区存储。<strong>Hadoop提供了一个综合性的文件系统抽象</strong>， Hadoop Distributed FileSystem 简称 HDFS或者DFS，Hadoop 分布式文件系统。它是Hadoop 3大组件之一。其他两大组件为 Hadoop-common 和 Hadoop-mapreduce。</p>
<p>传统以文件为基本单位的存储缺点：首先它很难实现并行化处理某个文件。单个节点一次只能处理一个文件，无法同时处理其他文件；再者，文件大小不同很难实现负载均衡。</p>
<h3>HDFS的设计</h3>
<ul>
<li>HDFS以流式数据访问模式来存储超大文件，部署运行于廉价的机器上。</li>
<li>可存储超大文件；流式访问，一次写入，多次读取；商用廉价PC,并不需要高昂的高可用的硬件。</li>
<li>但不适用于，低时间延迟的访问；大小文件处理（浪费namenode内存，浪费磁盘空间。）；多用户写入，任意修改文件(不支持并发写入。 同一时刻只能一个进程写入，不支持随机修改。)。</li>
</ul>
<h3>数据块</h3>
<p>块是磁盘进行数据读写的最小单位，默认是512字节，构建单个磁盘之上的文件系统通过磁盘块来管理文件系统的来管理该文件系统中的块。HDFS的块默认是64MB,<strong>HDFS上的文件也被划分为块大小的多个分块(chunk)，作为独立的存储单元</strong>。HDFS块默认64MB的好处是为了简化<a href="https://community.emc.com/thread/149881">磁盘寻址</a>的开销。</p>
<h3>HDFS块的抽象好处</h3>
<ul>
<li><strong>一个文件的大小，可以大于网络中任意一个硬盘的大小</strong>。文件的块并不需要存储在同一个硬盘上可以存储在分布式文件系统集群中任意一个硬盘上。</li>
<li><strong>大大简化系统设计</strong>。这点对于故障种类繁多的分布式系统来说尤为重要。以块为单位，不仅简化存储管理（块大小是固定的，很容易计算一个硬盘放多少个块）；而且，消除了元数据的顾虑（因为Block仅仅是存储的一块数据，其文件的元数据，例如权限等就不需要跟数据块一起存储，可以交由另外的其他系来处理）。适合批处理。支持离线的批量数据处理，支持高吞吐量。</li>
<li><strong>块更适合于数据备份</strong>。进而提供数据容错能力和系统可用性（将每个块复制至少几个独立的机器上，可以确保在发生块、磁盘或机器故障后数据不丢失。一旦发生某个块不可用，系统将从其他地方复制一份复本。以保证复本的数量恢复到正常水平）。容错性高，容易实现负载均衡。</li>
</ul>
<h3>Namenode 和 Datanode</h3>
<p>HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的 Datanodes 组成。<strong>Namenode</strong>是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的<strong>Datanode</strong>一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的namespace操作。比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的 映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。</p>
<p>NameNode上维护文件系统树及整棵树内所有的文件和目录，并永久保存在本地磁盘，fsimage和editslog。</p>
<p>NameNode 将对文件系统的改动追加保存到本地文件系统上的一个日志文件（edits）。当一个 NameNode 启动时，它首先从一个映像文件（fsimage）中读取 HDFS 的状态，接着应用日志文件中的 edits 操作。然后它将新的 HDFS 状态写入（fsimage）中，并使用一个空的 edits 文件开始正常操作。因为 NameNode 只有在启动阶段才合并 fsimage 和 edits，所以久而久之日志文件可能会变得非常庞大，特别是对大型的集群。日志文件太大的副作用是下一次 NameNode 启动会花很长时间。</p>
<h4>Hadoop HDFS 架构图</h4>
<p><img src="http://zhaomingtai.u.qiniudn.com/hdfs-architecture.gif" alt="image" /></p>
<p>在上图中NameNode是Master上的进程，复制控制底层文件的io操作，处理mapreduce任务等。
DataNode运行在slave机器上，负责实际的地层文件io读写。由NameNode存储管理文件系统的命名空间。</p>
<p><strong>客户端</strong>代表用户通过与NameNode和DataNode交互来访问整个文件系统。</p>
<h4>HDFS 读过程</h4>
<p><img src="http://zhaomingtai.u.qiniudn.com/hdfs-read.png" alt="image" /></p>
<ol>
<li>客户端通过调用 FileSystem 对象的 open() 方法来打开要读取的文件。（步骤 1）在HDFS中是个 DistributedFileSystem 中的一个实例对象。</li>
<li>（步骤 2）DistributedFileSystem 通过PRC[需要提供一个外链接介绍RPC技术]调用 namenode ,获取文件起始块的位置。对于每个块，namenode 返回存有该块复本的 datanode 地址。Datanode 根据它们于该客户端的距离排序，如果该客户端就是一个 datanode 并保存有该数据块的一个复本，该节点就直接从本地 datanode 中读取数据。反之取网路拓扑最短路径。</li>
<li>DistributedFileSystem 返回 FSDataInputStream 给客户端，用来读取数据。FSDataInputStream  类封装 DFSInputStream 对象。由 DFSInputStream 负责管理 DataNode 和 NameNode 的 I/O。</li>
<li>（步骤 3）客户端调用 stream 的 read() 函数开始读取数据。</li>
<li>存储着文件起始块的 DataNode 地址的 DFSInputStream 随即连接距离最近的 DataNode。反复 read() 方法，将数据从 datanode 传输到客户端。（网络拓扑与Hadoop[机架感知] ？ 连接待补。）</li>
<li>到达块的末端时，DFSInputStream 关闭和此数据节点的连接，然后连接此文件下一个数据块的最佳DataNode。</li>
<li>客户端读取数据时，块也是按照打开 DFSInputStream 与 datanode 建立连接的顺序读取的。以通过询问NameNode 来检索下一批所需块的 datanode。当客户端读取完毕数据的时候，调用 FSDataInputStream 的 close() 函数。</li>
<li>在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。</li>
</ol>
<h5>源代码理解</h5>
<ol>
<li><p>在客户端执行 class DistributedFileSystem open() 方法（装饰模式），打开文件并返回 DFSInputStream。</p>
<pre class='brush:java'>// DistributedFileSystem extends FileSystem --&gt; 调用 open() 方法
public FSDataInputStream open(Path f, int bufferSize) throws IOException {
    statistics.incrementReadOps(1);
    return new DFSClient.DFSDataInputStream(
        dfs.open(getPathName(f), bufferSize, verifyChecksum, statistics));
}
</pre><pre class='brush:java'>// dfs 是 DFSClient 的实例对象。
public DFSInputStream open(String src, int buffersize, boolean verifyChecksum,
               FileSystem.Statistics stats
) throws IOException {
    checkOpen(); 
    //    Get block info from namenode
    return new DFSInputStream(src, buffersize, verifyChecksum);
}
</pre><pre class='brush:java'>// DFSInputStream 是 DFSClient 的内部类，继承自 FSInputStream。
// 调用的构造函数（具体略）中调用了 openInfo() 方法。在 openInfo() 中 重要的是 fetchLocatedBlocks() 向 NameNode 询问所需要的数据的元信息，通过 callGetBlockLocations() 实现。 此过程若没有找到将尝试3次。 
//
// 由 callGetBlockLocations()通过 RPC 方式询问 NameNode 获取到 LocatedBlocks 信息。
static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,
  String src, long start, long length) throws IOException {
… … 
    return namenode.getBlockLocations(src, start, length);
… … 
}    
</pre><pre class='brush:java'>// 此处的 namenode 是通过代理模式创建的。它是 namenode  ClientProtocol 的实现（interface ClientProtocol extends VersionedProtocol）。
private static ClientProtocol createNamenode(ClientProtocol rpcNamenode,
Configuration conf) throws IOException {
… … 
    final ClientProtocol cp = (ClientProtocol)RetryProxy.create(ClientProtocol.class, rpcNamenode, defaultPolicy, methodNameToPolicyMap);
    RPC.checkVersion(ClientProtocol.class, ClientProtocol.versionID, cp);
… … 
}  
</pre></li>
<li><p>在 class NameNode 端获取数据块位置信息并排序</p>
<pre class='brush:java'>public LocatedBlocks   getBlockLocations(String src, long offset, long length) throws IOException {
    myMetrics.incrNumGetBlockLocations();
    // 获取数据块信息。namenode 为 FSNamesystem 实例。
    // 保存的是NameNode的name space树，其属性 FSDirectory dir 关联着 FSImage fsimage 信息，
    // fsimage 关联 FSEditLog editLog。
    return namesystem.getBlockLocations(getClientMachine(), src, offset, length);
}
</pre><pre class='brush:java'>// 类 FSNamesystem.getBlockLocationsInternal() 是具体获得块信息的实现。
private synchronized LocatedBlocks getBlockLocationsInternal(String src,
long offset, long length, int nrBlocksToReturn, 
boolean doAccessTime,  boolean needBlockToken) throws IOException {
    … … 
}  
</pre></li>
<li><p>在客户端DFSClient将步骤1中打开的读文件， DFSDataInputStream 对象内部的 DFSInputStream 对象的 read(long position, byte[] buffer, int offset, int length)方法进行实际的文件读取</p>
<pre class='brush:java'>// class DFSInputStream
public int read(long position, byte[] buffer, int offset, int length)
  throws IOException {
  // sanity checks
  checkOpen();
  if (closed) {
    throw new IOException("Stream closed");
  }
  failures = 0;
  long filelen = getFileLength();
  if ((position &lt; 0) || (position &gt;= filelen)) {
    return -1;
  }
  int realLen = length;
  if ((position + length) &gt; filelen) {
    realLen = (int)(filelen - position);
  }
  //
  // determine the block and byte range within the block
  // corresponding to position and realLen
  // 判断块内的块和字节范围,位置和实际的长度
  List&lt;LocatedBlock&gt; blockRange = getBlockRange(position, realLen);
  int remaining = realLen;
  for (LocatedBlock blk : blockRange) {
    long targetStart = position - blk.getStartOffset();
    long bytesToRead = Math.min(remaining, blk.getBlockSize() - targetStart);
    fetchBlockByteRange(blk, targetStart, 
                        targetStart + bytesToRead - 1, buffer, offset);
    remaining -= bytesToRead;
    position += bytesToRead;
    offset += bytesToRead;
  }
  assert remaining == 0 : "Wrong number of bytes read.";
  if (stats != null) {
    stats.incrementBytesRead(realLen);
  }
  return realLen;
} 
</pre><pre class='brush:java'>// fetchBlockByteRange() 通过 socket 连接一个最优的 DataNode 来读取数据
private void fetchBlockByteRange(LocatedBlock block, long start,
                                 long end, byte[] buf, int offset) throws IOException {
  //
  // Connect to best DataNode for desired Block, with potential offset
  //
  Socket dn = null;
  int refetchToken = 1; // only need to get a new access token once
  //      
  while (true) {
    // cached block locations may have been updated by chooseDataNode()
    // or fetchBlockAt(). Always get the latest list of locations at the 
    // start of the loop.
    block = getBlockAt(block.getStartOffset(), false);
    DNAddrPair retval = chooseDataNode(block); // 选者最DataNode
    DatanodeInfo chosenNode = retval.info;
    InetSocketAddress targetAddr = retval.addr;
    BlockReader reader = null;
    try {
      Token&lt;BlockTokenIdentifier&gt; accessToken = block.getBlockToken();
      int len = (int) (end - start + 1);
  //
      // first try reading the block locally.
      if (shouldTryShortCircuitRead(targetAddr)) {// 本地优先
        try {
          reader = getLocalBlockReader(conf, src, block.getBlock(),
              accessToken, chosenNode, DFSClient.this.socketTimeout, start);
        } catch (AccessControlException ex) {
          LOG.warn("Short circuit access failed ", ex);
          //Disable short circuit reads
          shortCircuitLocalReads = false;
          continue;
        }
      } else {
        // go to the datanode
        dn = socketFactory.createSocket(); // socke datanode
        LOG.debug("Connecting to " + targetAddr);
        NetUtils.connect(dn, targetAddr, getRandomLocalInterfaceAddr(),
            socketTimeout);
        dn.setSoTimeout(socketTimeout);
        reader = RemoteBlockReader.newBlockReader(dn, src, 
            block.getBlock().getBlockId(), accessToken,
            block.getBlock().getGenerationStamp(), start, len, buffersize, 
            verifyChecksum, clientName);
      }
      int nread = reader.readAll(buf, offset, len); // BlockReader 负责读取数据
      return;
    }
    … … 
    finally {
      IOUtils.closeStream(reader);
      IOUtils.closeSocket(dn);
    }
    // Put chosen node into dead list, continue
    addToDeadNodes(chosenNode); // dead datanode
  }
}
</pre></li>
<li><p>NameNode 实例化启动时便监听客户端请求</p>
<pre class='brush:java'>DataNode(final Configuration conf,
       final AbstractList&lt;File&gt; dataDirs, SecureResources resources) throws IOException {
super(conf);
SecurityUtil.login(conf, DFSConfigKeys.DFS_DATANODE_KEYTAB_FILE_KEY, 
    DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY);
//
datanodeObject = this;
durableSync = conf.getBoolean("dfs.durable.sync", true);
this.userWithLocalPathAccess = conf
    .get(DFSConfigKeys.DFS_BLOCK_LOCAL_PATH_ACCESS_USER_KEY);
try {
  startDataNode(conf, dataDirs, resources);// startDataNode
} catch (IOException ie) {
  shutdown();
  throw ie;
}   
}
</pre><pre class='brush:java'>// startDataNode
void startDataNode(Configuration conf, 
                 AbstractList&lt;File&gt; dataDirs, SecureResources resources
                 ) throws IOException {
… …                      
// find free port or use privileged port provide
ServerSocket ss;
if(secureResources == null) {
  ss = (socketWriteTimeout &gt; 0) ? 
    ServerSocketChannel.open().socket() : new ServerSocket();
  Server.bind(ss, socAddr, 0);
} else {
  ss = resources.getStreamingSocket();
}
ss.setReceiveBufferSize(DEFAULT_DATA_SOCKET_SIZE); 
// adjust machine name with the actual port
tmpPort = ss.getLocalPort();
selfAddr = new InetSocketAddress(ss.getInetAddress().getHostAddress(),
//                                     tmpPort);
this.dnRegistration.setName(machineName + ":" + tmpPort);
LOG.info("Opened data transfer server at " + tmpPort);
//
this.threadGroup = new ThreadGroup("dataXceiverServer");
this.dataXceiverServer = new Daemon(threadGroup, 
    new DataXceiverServer(ss, conf, this));
this.threadGroup.setDaemon(true); // DataXceiverServer为守护线程监控客户端连接
}
</pre><pre class='brush:java'>// class DataXceiverServer.run()
public void run() {
while (datanode.shouldRun) {
  try {
    Socket s = ss.accept();
    s.setTcpNoDelay(true);
    new Daemon(datanode.threadGroup, 
        new DataXceiver(s, datanode, this)).start();
  } catch (SocketTimeoutException ignored) {
  }
}
}   
</pre><pre class='brush:java'>// class DataXceiver.run()
// Read/write data from/to the DataXceiveServer.
// 操作类型：OP_READ_BLOCK,OP_WRITE_BLOCK,OP_REPLACE_BLOCK,
// OP_COPY_BLOCK,OP_BLOCK_CHECKSUM
public void run() {
DataInputStream in=null; 
try {
  in = new DataInputStream(
      new BufferedInputStream(NetUtils.getInputStream(s), 
                              SMALL_BUFFER_SIZE));
… … 
  switch ( op ) {
  case DataTransferProtocol.OP_READ_BLOCK:
    readBlock( in );// 读数据
    datanode.myMetrics.addReadBlockOp(DataNode.now() - startTime);
    if (local)
      datanode.myMetrics.incrReadsFromLocalClient();
    else
      datanode.myMetrics.incrReadsFromRemoteClient();
    break;
… … 
  default:
    throw new IOException("Unknown opcode " + op + " in data stream");
  }
}  
</pre><pre class='brush:java'>// class DataXceiver.readBlock()
// Read a block from the disk.
private void readBlock(DataInputStream in) throws IOException {
//
// Read in the header,读指令
//
long blockId = in.readLong();          
Block block = new Block( blockId, 0 , in.readLong());
// 
long startOffset = in.readLong();
long length = in.readLong();
String clientName = Text.readString(in);
Token&lt;BlockTokenIdentifier&gt; accessToken = new Token&lt;BlockTokenIdentifier&gt;();
accessToken.readFields(in);
// 向客户端写数据
OutputStream baseStream = NetUtils.getOutputStream(s, 
    datanode.socketWriteTimeout);
DataOutputStream out = new DataOutputStream(
             new BufferedOutputStream(baseStream, SMALL_BUFFER_SIZE));
… … 
// send the block,读取本地的block的数据，并发送给客户端
BlockSender blockSender = null;
final String clientTraceFmt =
  clientName.length() &gt; 0 &amp;&amp; ClientTraceLog.isInfoEnabled()
    ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,
        "%d", "HDFS_READ", clientName, "%d", 
        datanode.dnRegistration.getStorageID(), block, "%d")
    : datanode.dnRegistration + " Served " + block + " to " +
        s.getInetAddress();
try {
  try {
    blockSender = new BlockSender(block, startOffset, length,
        true, true, false, datanode, clientTraceFmt);
  } catch(IOException e) {
    out.writeShort(DataTransferProtocol.OP_STATUS_ERROR);
    throw e;
  }
  out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS); // send op status
  long read = blockSender.sendBlock(out, baseStream, null); // send data,发送数据
  … … 
} finally {
  IOUtils.closeStream(out);
  IOUtils.closeStream(blockSender);
}
}
</pre></li>
</ol>
<h4>HDFS 写过程</h4>
<p><img src="http://zhaomingtai.u.qiniudn.com/hdfs-write.png" alt="image" /></p>
<ol>
<li>客户端通过对 DistributedFileSystem 对象调用 create() 方法来创建文件（步骤1）。</li>
<li>DistributedFileSystem 通过 PRC 对 namenode 调用create() 方法，在文件系统的命名空间中创建一个新的没有数据块文件（步骤2）。</li>
<li>namenode 检查并确保此文件不存在，并且客户端由创建该文件的权限。通过 namenode 即为创建的新文件创建一条纪录；否则，创建失败，并向客户端抛出 IOException 异常。</li>
<li>DistributedFileSystem 向客户端返回一个 FSDataOutputStream 对象。客户端可以开始写数据。FSDataOutputStream 同样封装一个 DFSoutPutstream 对象。由 DFSInputStream 负责处理 DataNode 和 NameNode 的 I/O。</li>
<li>（步骤3）在客户端写数据时，DFSoutPutstream 将它分成一个个的数据包，并写入内部队列（数据队列）。</li>
<li>由 DataStreamer(DFSClient 内部类) 处理数据队列。它根据 datanode 列表要求 namenode 分配适合的新块来存储数据备份。这组 datanode 构成一个管线。假设当前复制数为3，那么管线中将有3个节点。DataStreamer 将数据包流式传输到管线（pipeline）的第一个 datanode 节点。该 datanode 存储数据包并将它发送到管线中的第2个 datanode。 同样地，第二个 datanode 存储该数据包并发哦少年宫到管线中的第3个 datanode（步骤4）。</li>
<li>DFSOutputStream 内部维护一个对应的数据包队列等待 datanode 收到确认确认回执(ack queue)，当 DFSOutputStream 收到所有的 datanode 确认信息之后，该数据包才从确认队列中删除。</li>
<li>若在写数据时，datanode 发生故障。则先关闭管线，确认把队列中任何数据包都添加回数据队列的最前端，以确保故障节点下游的 datanode 不会漏掉任何一个数据包。并为存储在另一个 datanode 的当前数据块指定一个新的标志，并将该标志发送个 namenode，以便故障的 datanode 在恢复后可以删除存储的部分数据块。从管线中删除故障 datanode 节点并把余下的数据块写入管线中的2个 datanode。Namenode 注意到块复本量不足时，会在另一个节点上创建一个新的复本。后续数据块继续正常处理。一个块在写入期间发生多个 datanode 故障的概率不高，只要写入了最小复本数（dfs.replication.min默认为1），写入即为成功。此块由异步执行复制以达到目标复本数，默认为3。</li>
<li>当客户端结束写入数据，则调用 stream 的 close()函数。此操作将剩余所有的数据包写入 datanode  pipeline 中，并等待 ack queue 返回成功。最后通知元数据节点写入完毕。namenode 是通过 Datastreamer 询问的数据块的分配，它在返回成功前只需要等待数据块进行最小量的复制。</li>
</ol>
<h5>源代码理解</h5>
<p>TODO，原笔记已丢失，待补。
hdfs 架构一页。且读过程和写过程各独立一页。</p>
<h4>NadeNode 和 DataNode 实现的协议</h4>
<p>TODO ，独立 一页</p>
<h4>补充</h4>
<p>详细介绍<a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/10/1874222.html">HDFS读写过程解析</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hadoop1.x 学习准备]]></title>
    <link href="http://kangfoo.u.qiniudn.com//article/2014/02/hadoop1.x--xue-xi-zhun-bei/"/>
    <updated>2014-02-24T23:28:00+08:00</updated>
    <id>http://kangfoo.u.qiniudn.com//article/2014/02/hadoop1.x--xue-xi-zhun-bei/</id>
    <content type="html"><![CDATA[<p>开始学习Hadoop之前先了解下<a href="http://www.programmer.com.cn/15929/">Hadoop之父Doug Cutting</a> 膜拜是必须的。路子是一步不走出来了。各位前辈给我们留下了宝贝的资源。不加以学习利用有些说不过去。</p>
<p>我个人是从2013年夏天开始拿到 《Hadoop权威指南》第二版的，但由于各种原因，也可以直接说我比较懒，从夏天到冬季中途到是翻过那书，感觉每次都没由什么实际的记忆和理解过程。在12月初，发现<a href="http://new.osforce.cn/course/explore/bigdata">开源力量</a>提供了相关的网上视频在线教学课程。选择适合自己的就是对的。
，我毅然成为了其中的一员。
目前我没有从事相关的工作，出于在工作之外寻找乐趣，就来了。</p>
<p>学习之前还是多看些相关的资源比较容易找到赶脚。</p>
<h6>资源列举：</h6>
<ul>
<li><a href="http://hadoopbook.com/">hadoop book</a></li>
<li><a href="https://github.com/tomwhite/hadoop-book">hadoop 权威指南源代码</a></li>
<li><a href="http://hadoop.apache.org/docs/r1.2.1/">Hadoop 1.2.1 官方网站</a></li>
<li><a href="http://www.cloudera.com/content/support/en/documentation/cdh4-documentation/cdh4-documentation-v4-latest.html">CDH 版本 Hadoop</a></li>
<li><a href="http://blog.cloudera.com/blog/">cloudera blog</a></li>
<li><a href="http://hortonworks.com/blog/">hortonworks blog</a></li>
<li><a href="https://issues.apache.org/jira/secure/BrowseProjects.jspa#all">apache jira</a></li>
<li><a href="http://dongxicheng.org/">董的博客</a></li>
<li><a href="http://wenku.baidu.com/view/ffc10130eefdc8d376ee32ec.html">Hadoop源码分析－百度文库</a></li>
<li><a href="http://wenku.baidu.com/link?url=TUHOh2GOIBfJx4KDgmlXP7YwhvCaZv8OLPHBzkqDXNL5pvI_qrQ8c0AjAP7_d931KkrHAPM7Hocifq1iEnaSjOxQcYghgYNs4ocPVhNNX0y">Google三大论文－百度文库（英文版本的我总是打不开，待补）</a></li>
<li><a href="http://grepcode.com/search/?query=hadoop&amp;entity=project">在线查看源代码</a></li>
<li><a href="http://cloud.ozyegin.edu.tr/Hadoop-UML-Diagrams/Documentation/html/d1/d25/interfaceorg_1_1apache_1_1hadoop_1_1hdfs_1_1server_1_1protocol_1_1_namenode_protocol.html">在线绘制UML图等</a></li>
</ul>
<p>部分网站可能是比较难打开的。经常遇到问题谷歌之后发现很多问题还是跳转到这些权威论坛上面了。先登记在案，以备使用。在此仅以纪录我学习《Hadoop权威指南》一书的笔记。
笔记中图片资源大部分出自于原书英文版，章节内容来自于原书中/英文版 ＋ 开源力量 LouisT 老师ppt/课堂示例及扩展，当然不乏在网站上找到各位博主的精品博客参考。</p>
<p>我自己的练习代码存放在 <a href="https://github.com/kangfoo/hadoop1.study/tree/master#hadoop1study">github。</a></p>
]]></content>
  </entry>
</feed>
